@article{Agostino,
author = {Agostino, Antonello D and Mechanism, European Stability and Giannone, Domenico and Reserve, Federal and York, New and Lenza, Michele and Bank, European Central and Modugno, Michele and Board, Federal Reserve},
keywords = {and do not necessarily,and the federal reserve,busi-,current economic conditions,dynamic factor models,dynamic heterogeneity,european central bank or,eurosystem,ness cycles,nowcasting,real time,reflect those of the,supported by the,system,the,the european stability mechanism,the views expressed are,this work was partly,those of the authors},
pages = {1--24},
title = {{Nowcasting Business Cycles : a Bayesian Approach to Dynamic Heterogeneous Factor Models}}
}
@article{Alessi2014,
author = {Alessi, Lucia},
file = {:E$\backslash$:/Dropbox/phd/forecast/Central Bank Macroeconomic Forecasting.pdf:pdf},
number = {680},
title = {{Central Bank Macroeconomic Forecasting during the Global Financial Crisis : The European Central Bank and Federal Reserve Bank of New York Experiences}},
year = {2014}
}
@article{Altissimo2010,
abstract = {Removal of short-run dynamics from a stationary time series to isolate the medium- to long-run component can be obtained by a bandpass filter. However, bandpass filters are infinite moving averages and can therefore deteriorate at the end of the sample. This is a well-known result in the literature isolating the business cycle in integrated series. We show that the same problem arises with our application to stationary time series. In this paper, we develop a method to obtain smoothing of a stationary time series by using only contemporaneous values of a large data set, so that no end-of-sample deterioration occurs. Our method is applied to the construction of New Eurocoin, an indicator of economic activity for the euro area, which is an estimate, in real time, of the medium- to long-run component of GDP growth. As our data set is monthly and most of the series are updated with a short delay, we are able to produce a monthly real-time indicator. As an estimate of the medium- to long-run GDP growth, Eurocoin performs better than the bandpass filter at the end of the sample in terms of both fitting and turning-point signaling. (c) 2010 The President and Fellows of Harvard College and the Massachusetts Institute of Technology.},
author = {Altissimo, Filippo and Cristadoro, Riccardo and Forni, Mario and Lippi, Marco and Veronese, Giovanni},
file = {:E$\backslash$:/Dropbox/phd/MIDAS/UMIDAS/New Eurocoin Tracking Economic Growth in Real Time.pdf:pdf},
journal = {The Review of Economics and Statistics},
number = {4},
pages = {1024--1034},
publisher = {MIT Press},
title = {{New Eurocoin: Tracking Economic Growth in Real Time}},
url = {http://ideas.repec.org/a/tpr/restat/v92y2010i4p1024-1034.html},
volume = {92},
year = {2010}
}
@incollection{Andersson2008,
abstract = {We consider forecast combination and, indirectly, model selection for VAR models when there is uncertainty about which variables to include in the model in addition to the forecast variables. The key difference from traditional Bayesian variable selection is that we also allow for uncertainty regarding which endogenous variables to include in the model. That is, all models include the forecast variables, but may otherwise have differing sets of endogenous variables. This is a difficult problem to tackle with a traditional Bayesian approach. Our solution is to focus on the forecasting performance for the variables of interest and we construct model weights from the predictive likelihood of the forecast variables. The procedure is evaluated in a small simulation study and found to perform competitively in applications to real world data.},
annote = {doi:10.1016/S0731-9053(08)23015-X},
author = {Andersson, Michael K and Karlsson, Sune},
booktitle = {Bayesian Econometrics},
doi = {doi:10.1016/S0731-9053(08)23015-X},
month = jan,
pages = {501--524},
publisher = {Emerald Group Publishing Limited},
series = {Advances in Econometrics},
title = {{Bayesian forecast combination for VAR models}},
url = {http://dx.doi.org/10.1016/S0731-9053(08)23015-X},
volume = {23},
year = {2008}
}
@article{Andersson2007,
abstract = {We consider forecast combination and, indirectly, model selection for VAR models when there is uncertainty about which variables to include in the model in addition to the forecast variables. The key difference from traditional Bayesian variable selection is that we also allow for uncertainty regarding which endogenous variables to include in the model. That is, all models include the forecast variables, but may otherwise have differing sets of endogenous variables. This is a difficult problem to tackle with a traditional Bayesian approach. Our solution is to focus on the forecasting performance for the variables of interest and we construct model weights from the predictive likelihood of the forecast variables. The procedure is evaluated in a small simulation study and found to perform competitively in applications to real world data.},
annote = {From Duplicate 1 (Bayesian forecast combination for VAR models - Andersson, Michael K; Karlsson, Sune)},
author = {Andersson, Michael K and Karlsson, Sune},
doi = {doi:10.1016/S0731-9053(08)23015-X},
file = {:C$\backslash$:/Users/snowdj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Andersson, Karlsson - 2007 - Bayesian forecast combination for VAR models.pdf:pdf},
journal = {Working Paper Series},
keywords = {Bayesian model averaging,GDP forecasts,Predictive likelihood},
month = nov,
pages = {501--524},
publisher = {Sveriges Riksbank (Central Bank of Sweden)},
series = {Advances in Econometrics},
title = {{Bayesian forecast combination for VAR models}},
url = {http://dx.doi.org/10.1016/S0731-9053(08)23015-X http://ideas.repec.org/p/hhs/rbnkwp/0216.html},
volume = {23},
year = {2007}
}
@techreport{Andreou2010,
author = {Andreou, Elena and Ghysels, Eric and Kourtellos, Andros},
month = nov,
publisher = {University of Cyprus Department of Economics},
title = {{Forecasting with mixed-frequency data}},
url = {http://econpapers.repec.org/RePEc:ucy:cypeua:10-2010},
year = {2010}
}
@article{Andreou2013a,
author = {Andreou, Elena and Ghysels, Eric and Kourtellos, Andros},
doi = {10.1080/07350015.2013.767199},
file = {:C$\backslash$:/Users/snowdj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Andreou, Ghysels, Kourtellos - 2013 - Should Macroeconomic Forecasters Use Daily Financial Data and How(3).pdf:pdf},
isbn = {0735-0015$\backslash$r1537-2707},
issn = {0735-0015},
journal = {Journal of Business \& Economic Statistics},
keywords = {daily financial factors,financial markets and the,macroeconomy,midas regressions},
number = {2},
pages = {240--251},
title = {{Should Macroeconomic Forecasters Use Daily Financial Data and How?}},
url = {http://www.tandfonline.com/doi/abs/10.1080/07350015.2013.767199},
volume = {31},
year = {2013}
}
@techreport{Avi2015,
abstract = {As the cost of storing, sharing, and analyzing data has decreased, economic activity has become increasingly digital. But while the effects of digital technology and improved digital communication have been explored in a variety of contexts, the impact on economic activity—from consumer and entrepreneurial behavior to the ways in which governments determine policy—is less well understood. Economics of Digitization explores the economic impact of digitization, with each chapter identifying a promising new area of research. The Internet is one of the key drivers of growth in digital communication, and the first set of chapters discusses basic supply-and-demand factors related to access. Later chapters discuss new opportunities and challenges created by digital technology and describe some of the most pressing policy issues. As digital technologies continue to gain in momentum and importance, it has become clear that digitization has features that do not fit well into traditional economic models. This suggests a need for a better understanding of the impact of digital technology on economic activity, and Economic Analysis of the Digital Economy brings together leading scholars to explore this emerging area of research.},
author = {{Avi Goldfarb} and {Shane Greenstein} and {Catherine Tucker} and Goldfarb, Avi and Greenstein, Shane M. and Tucker, Catherine E.},
institution = {National Bureau of Economic Research},
isbn = {9780226206844},
publisher = {University of Chicago Press},
title = {{Economic Analysis of the Digital Economy}},
url = {http://econpapers.repec.org/RePEc:ucp:bknber:9780226206844 http://www.nber.org/books/gree13-1},
year = {2015}
}
@article{Ba2011,
author = {Ba, Marta and Giannone, Domenico},
title = {{Nowcasting with daily data}},
year = {2011}
}
@article{Bai2002,
abstract = {In this paper we develop some econometric theory for factor models$\backslash$nof large dimensions. The focus is the determination of the number$\backslash$nof factors (r), which is an unresolved issue in the rapidly growing$\backslash$nliterature on multifactor models. We first establish the convergence$\backslash$nrate for the factor estimates that will allow for consistent estimation$\backslash$nof r. We then propose some panel criteria and show that the number$\backslash$nof factors can be consistently estimated using the criteria. The$\backslash$ntheory is developed under the framework of large cross-sections (N)$\backslash$nand large time dimensions (T). No restriction is imposed on the relation$\backslash$nbetween N and T. Simulations show that the proposed criteria have$\backslash$ngood finite sample properties in many configurations of the panel$\backslash$ndata encountered in practice.},
author = {Bai, Jushan and Ng, Serena},
doi = {10.1111/1468-0262.00273},
isbn = {0012-9682},
issn = {0012-9682},
journal = {Econometrica},
keywords = {asset pricing,factor analysis,model selection,principal components},
number = {1},
pages = {191--221},
title = {{Determining the Number of Factors in Approximate Factor Models}},
volume = {70},
year = {2002}
}
@article{Bai2009,
author = {Bai, Jushan and Ng, Serena},
doi = {10.1002/jae.1063},
issn = {08837252},
journal = {Journal of Applied Econometrics},
month = jun,
number = {4},
pages = {607--629},
title = {{Boosting diffusion indices}},
url = {http://doi.wiley.com/10.1002/jae.1063},
volume = {24},
year = {2009}
}
@article{Bai2007,
author = {Bai, Jushan and Ng, Serena},
journal = {Journal of Business \& Economic Statistics},
pages = {52--60},
publisher = {American Statistical Association},
title = {{Determining the Number of Primitive Shocks in Factor Models}},
url = {http://econpapers.repec.org/RePEc:bes:jnlbes:v:25:y:2007:p:52-60},
volume = {25},
year = {2007}
}
@article{Bai2007a,
abstract = {A widely held but untested assumption underlying macroeconomic analysis is that the number of shocks driving economic fluctuations, q, is small. In this paper, we associate q with the number of dynamic factors in a large panel of data. We propose a methodology to determine q without having to estimate the dynamic factors. Our analysis is based on the residuals of a VAR in r static factors, where the factors are themselves obtained by applying the method of principal components to a large panel of data. Our algebraic tests are based on a spectral decomposition of the covariance matrix of the residuals of a VAR. The tests are exact if the residuals were observable. Because of sampling variability fromhaving to estimate the VAR, the tests are accurate up to an error that vanishes asymptotically. We show how this error depends on the rate of convergence of the sample covariance to the population covariance of the true innovations. An important aspect of the present analysis is to make precise the relation between the dynamic factors and the static factors, which is a result of independent interest.},
author = {Bai, Jushan and Ng, Serena},
doi = {10.1198/073500106000000413},
file = {:E$\backslash$:/Dropbox/phd/nowcast/factormodel/27638906.pdf:pdf},
issn = {0735-0015},
journal = {Journal of Business \& Economic Statistics},
number = {1},
pages = {52--60},
title = {{Determining the Number of Primitive Shocks in Factor Models}},
volume = {25},
year = {2007}
}
@article{Bai2012,
author = {Bai, Jushan and Wang, Peng},
doi = {10.1080/07350015.2014.941467},
keywords = {10027,columbia university,department of economics,dynamic factor models,impulse,multi-level factor model,new york,ny,response function,spill-over effects},
pages = {1--60},
title = {{Identification and Estimation of Dynamic Factor Models}},
year = {2012}
}
@incollection{Banbura2013a,
author = {Banbura, Marta and Giannone, Domenico and Modugno, Michele and Reichlin, Lucrezia},
booktitle = {Handbook of Economic Forecasting},
editor = {Elliott, Graham and Timmermann, Allan},
file = {:E$\backslash$:/Dropbox/phd/bsts/Now-casting and The real-time data flow.pdf:pdf},
keywords = {Macroeconomic news,macroeconomic forecasting,rea},
pages = {195--233},
publisher = {Elsevier},
title = {{Now-casting and the real-time data flow}},
year = {2013}
}
@article{BURA2010a,
author = {Banbura, Marta and Giannone, Domenico and Reichlin, Lucrezia},
issn = {01451707},
journal = {Journal of Applied Econometrics},
number = {25},
pages = {71--92},
title = {{Large Bayesian vector auto regression}},
volume = {25},
year = {2010}
}
@article{Banbura2010,
abstract = {We define nowcasting as the prediction of the present, the very near future and the very recent past. Crucial in this process is to use timely monthly information in order to nowcast key economic variables, such as e.g. GDP, that are typically collected at low frequency and published with long delays. Until recently, nowcasting had received very little attention by the academic literature, although it was routinely conducted in policy institutions either through a judgemental process or on the basis of simple models. We argue that the nowcasting process goes beyond the simple production of an early estimate as it essentially requires the assessment of the impact of new data on the subsequent forecast revisions for the target variable. We design a statistical model which produces a sequence of nowcasts in relation to the real time releases of various economic data. The methodology allows to process a large amount of information, as it is traditionally done by practitioners using judgement, but it does it in a fully automatic way. In particular, it provides an explicit link between the news in consecutive data releases and the resulting forecast revisions. To illustrate our ideas, we study the nowcast of euro area GDP in the fourth quarter of 2008.},
address = {Oxford},
author = {Banbura, Marta and Giannone, Domenico and Reichlin, Lucrezia},
editor = {Clements, M and Hendry, D},
journal = {Oxford Handbook on Economic Forecasting},
publisher = {Oxford University Press},
title = {{Nowcasting}},
year = {2010}
}
@article{Banbura2010a,
abstract = {In this paper we propose a methodology to estimate a dynamic factor model on data sets with an arbitrary pattern of missing data. We modify the Expectation Maximisation (EM) algorithm as proposed for a dynamic factor model by Watson and Engle (1983) to the case with general pattern of missing data. We also extend the model to the case with serially correlated idiosyncratic component. The framework allows to handle efficiently and in an automatic manner sets of indicators characterized by different publication delays, frequencies and sample lengths. This can be relevant e.g. for young economies for which many indicators are compiled only since recently. We also show how to extract a model based news from a statistical data release within our framework and we derive the relationship between the news and the resulting forecast revision. This can be used for interpretation in e.g. nowcasting applications as it allows to determine the sign and size of a news as well as its contribution to the revision, in particular in case of simultaneous data releases. We evaluate the methodology in a Monte Carlo experiment and we apply it to nowcasting and backdating of euro area GDP.},
author = {Banbura, Marta and Modugno, Michele},
doi = {10.1002/jae},
file = {:C$\backslash$:/Users/snowdj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/BAŃBURA, MODUGNO - 2014 - MAXIMUM LIKELIHOOD ESTIMATION OF FACTOR MODELS ON DATASETS WITH ARBITRARY PATTERN OF MISSING DATA.pdf:pdf},
issn = {01451707},
journal = {Journal of Applied Econometrics},
keywords = {Factor Models,Forecasting,Large Cross-Sections},
number = {4},
pages = {133--160},
title = {{Maximum likelihood estimation of factor models on datasets with arbitrary pattern of missing data}},
volume = {29},
year = {2014}
}
@article{Bates2013a,
abstract = {This paper considers the estimation of approximate dynamic factor models when there is temporal instability in the factor loadings. We characterize the type and magnitude of instabilities under which the principal components estimator of the factors is consistent and find that these instabilities can be larger than earlier theoretical calculations suggest. We also discuss implications of our results for the robustness of regressions based on the estimated factors and of estimates of the number of factors in the presence of parameter instability. Simulations calibrated to an empirical application indicate that instability in the factor loadings has a limited impact on estimation of the factor space and diffusion index forecasting, whereas estimation of the number of factors is more substantially affected. © 2013 Elsevier B.V. All rights reserved.},
author = {Bates, Brandon J. and Plagborg-M\o ller, Mikkel and Stock, James H. and Watson, Mark W.},
doi = {10.1016/j.jeconom.2013.04.014},
file = {:E$\backslash$:/Dropbox/phd/MIDAS/factors/Consistent factor estimation in dynamic factor models with structural instability.pdf:pdf},
isbn = {03044076},
issn = {03044076},
journal = {Journal of Econometrics},
month = dec,
number = {2},
pages = {289--304},
title = {{Consistent factor estimation in dynamic factor models with structural instability}},
url = {http://www.sciencedirect.com/science/article/pii/S0304407613000912},
volume = {177},
year = {2013}
}
@article{Baumeister2014,
author = {Baumeister, Christiane and Gu\'{e}rin, Pierre and Kilian, Lutz},
doi = {10.1016/j.ijforecast.2014.06.005},
issn = {0169-2070},
keywords = {Econometric and statistical methods,International topics},
title = {{Do High-Frequency Financial Data Help Forecast Oil Prices ? The MIDAS Touch at Work Do High-Frequency Financial Data Help Forecast Oil Prices ? The MIDAS Touch at Work}},
year = {2014}
}
@article{Bayesian2000,
author = {Bayesian, Applied},
number = {1997},
title = {{11 . Time series and dynamic linear models}},
year = {2000}
}
@article{Bec2013,
abstract = {This paper investigates the predictive accuracy of two alternative forecasting strategies, namely the forecast and information combinations. Theoretically, there should be no role for forecast combinations in a world where information sets can be instantaneously and costlessly combined. However, following some recent works which claim that this result holds in population but not necessarily in small samples, our paper questions this postulate empirically in a real-time and mixed-frequency framework. An application to the quarterly growth rate of French GDP reveals that, given a set of predictive models involving coincident indicators, a simple average of individual forecasts outperforms the individual forecasts, as long as no individual model encompasses the others. Furthermore, the simple average of individual forecasts outperforms, or it is statistically equivalent to, more sophisticated forecast combination schemes. However, when a predictive encompassing model is obtained by combining information sets, this model outperforms the most accurate forecast combination strategy.},
author = {Bec, Fr\'{e}d\'{e}rique and Mogliani, Matteo},
journal = {Working papers},
keywords = {Forecast Combinations,Macroeconomic Nowcasting,Mixed-frequency data.,Pooling Information,Real-time data},
title = {{Nowcasting French GDP in Real-Time from Survey Opinions: Information or Forecast Combinations?}},
url = {http://ideas.repec.org/p/bfr/banfra/436.html},
year = {2013}
}
@article{Bencivelli,
abstract = {This paper proposes the use of Bayesian model averaging (BMA) as a tool to select the predictors' set for bridge models. BMA is a computationally feasible method that allows us to explore the model space even in the presence of a large set of candidate predictors. We test the performance of BMA in now-casting by means of a recursive experiment for the euro area and the three largest countries. This method allows flexibility in selecting the information set month by month. We find that BMA based bridge models produce smaller forecast error than fixed composition bridges. In an application to the euro area they perform at least as well as medium-scale factor models.},
author = {Bencivelli, Lorenzo and Marcellino, Massimiliano and Moretti, Gianluca},
file = {:C$\backslash$:/Users/snowdj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bencivelli, Marcellino, Moretti - Unknown - Selecting predictors by using Bayesian model averaging in bridge models.pdf:pdf},
journal = {Temi di discussione (Economic working papers)},
keywords = {Bayesian model averaging,bridge models.,business cycle analysis,forecasting},
publisher = {Bank of Italy, Economic Research and International Relations Area},
title = {{Selecting predictors by using Bayesian model averaging in bridge models}},
url = {http://ideas.repec.org/p/bdi/wptemi/td\_872\_12.html},
year = {2012}
}
@article{Bencivelli2010,
author = {Bencivelli, Lorenzo and Marcellino, Massimiliano and Moretti, Gianluca},
journal = {Bank of Italy Temi di Discussione},
title = {{Temi di Discussione}},
year = {2010}
}
@article{Bilmes1998,
abstract = {We describe the maximum-likelihood parameter estimation problem and how the Expectation-Maximization (EM) algorithm can be used for its solution. We first describe the abstract form of the EM algorithm as it is often given in the literature. We then develop the EM parameter estimation procedure for two applications: 1) finding the parameters of a mixture of Gaussian densities, and 2) finding the parameters of a hidden Markov model (HMM) (i.e., the Baum-Welch algorithm) for both discrete and Gaussian mixture observation models. We derive the update equations in fairly explicit detail but we do not prove any convergence properties. We try to emphasize intuition rather than mathematical rigor.},
author = {Bilmes, Jeff a.},
doi = {10.1.1.119.4856},
file = {:E$\backslash$:/Dropbox/phd/statespace/em.pdf:pdf},
isbn = {0226775429},
journal = {International Computer Science Institute},
number = {510},
pages = {126},
title = {{A gentle tutorial of the EM algorithm and its application to parameter estimation for Gaussian mixture and hidden Markov models}},
volume = {4},
year = {1998}
}
@article{Blasques2014,
author = {Blasques, Francisco and Koopman, Siem Jan and Mallee, Max},
title = {{Low Frequency and Weighted Likelihood Solutions for Mixed Frequency Dynamic Factor Models}},
year = {2014}
}
@article{Boivin2005,
abstract = {Forecasting using "diffusion indices" has received a good deal of attention in recent years. The idea is to use the common factors estimated from a large panel of data to help forecast the series of interest. This paper assesses the extent to which the forecasts are influenced by (i) how the factors are estimated and/or (ii) how the forecasts are formulated. We find that for simple data-generating processes and when the dynamic structure of the data is known, no one method stands out to be systematically good or bad. All five methods considered have rather similar properties, though some methods are better in long-horizon forecasts, especially when the number of time series observations is small. However, when the dynamic structure is unknown and for more complex dynamics and error structures such as the ones encountered in practice, one method stands out to have smaller forecast errors. This method forecasts the series of interest directly, rather than the common and idiosyncratic components separately, and it leaves the dynamics of the factors unspecified. By imposing fewer constraints, and having to estimate a smaller number of auxiliary parameters, the method appears to be less vulnerable to misspecification, leading to improved forecasts.},
author = {Boivin, Jean and Ng, Serena},
file = {:C$\backslash$:/Users/snowdj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Boivin, Ng - 2005 - Understanding and Comparing Factor-Based Forecasts.pdf:pdf},
journal = {International Journal of Central Banking},
number = {3},
publisher = {International Journal of Central Banking},
title = {{Understanding and Comparing Factor-Based Forecasts}},
url = {http://ideas.repec.org/a/ijc/ijcjou/y2005q4a4.html},
volume = {1},
year = {2005}
}
@article{Boivin2006a,
abstract = {Factors estimated from large macroeconomic panels are being used in an increasing number of applications. However, little is known about how the size and the composition of the data affect the factor estimates. In this paper, we question whether it is possible to use more series to extract the factors, and yet the resulting factors are less useful for forecasting, and the answer is yes. Such a problem tends to arise when the idiosyncratic errors are cross-correlated. It can also arise if forecasting power is provided by a factor that is dominant in a small dataset but is a dominated factor in a larger dataset. In a real time forecasting exercise, we find that factors extracted from as few as 40 pre-screened series often yield satisfactory or even better results than using all 147 series. Weighting the data by their properties when constructing the factors also lead to improved forecasts. Our simulation analysis is unique in that special attention is paid to cross-correlated idiosyncratic errors, and we also allow the factors to have stronger loadings on some groups of series than others. It thus allows us to better understand the properties of the principal components estimator in empirical applications.},
author = {Boivin, Jean and Ng, Serena},
doi = {10.1016/j.jeconom.2005.01.027},
file = {:C$\backslash$:/Users/snowdj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Boivin, Ng - 2006 - Are more data always better for factor analysis.pdf:pdf},
issn = {03044076},
journal = {Journal of Econometrics},
keywords = {C33,C51,Factor models,Forecasting,Large N and T,Principal components},
month = may,
number = {1},
pages = {169--194},
publisher = {Elsevier},
title = {{Are more data always better for factor analysis?}},
url = {http://www.sciencedirect.com/science/article/pii/S030440760500045X http://ideas.repec.org/a/eee/econom/v132y2006i1p169-194.html},
volume = {132},
year = {2006}
}
@article{Brauning2014,
abstract = {We explore a new approach to the forecasting of macroeconomic variables based on a dynamic factor state space analysis. Key economic variables are modeled jointly with principal components from a large time series panel of macroeconomic indicators using a multivariate unobserved components time series model. When the key economic variables are observed at a low frequency and the panel of macroeconomic variables is at a high frequency, we can use our approach for both nowcasting and forecasting purposes. Given a dynamic factor model as the data generation process, we provide Monte Carlo evidence of the finite-sample justification of our parsimonious and feasible approach. We also provide empirical evidence for a US macroeconomic dataset. The unbalanced panel contains quarterly and monthly variables. The forecasting accuracy is measured against a set of benchmark models. We conclude that our dynamic factor state space analysis can lead to higher levels of forecasting precision when the panel size and time series dimensions are moderate.},
author = {Br\"{a}uning, Falk and Koopman, Siem Jan},
doi = {10.1016/j.ijforecast.2013.03.004},
file = {:C$\backslash$:/Users/snowdj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Br\"{a}uning, Koopman - 2014 - Forecasting macroeconomic variables using collapsed dynamic factor analysis.pdf:pdf},
issn = {01692070},
journal = {International Journal of Forecasting},
keywords = {Kalman filter,Maximum likelihood method,Principal components,State space dynamic factor model},
month = jul,
number = {3},
pages = {572--584},
title = {{Forecasting macroeconomic variables using collapsed dynamic factor analysis}},
url = {http://www.sciencedirect.com/science/article/pii/S0169207013000459},
volume = {30},
year = {2014}
}
@article{Breiman1996,
author = {Breiman, Leo},
doi = {10.1023/A:1018054314350},
issn = {0885-6125},
journal = {Machine Learning},
keywords = {aggregation,averaging,bootstrap,combining},
month = aug,
number = {2},
pages = {123--140},
publisher = {Kluwer Academic Publishers},
title = {{Bagging predictors}},
url = {http://dl.acm.org/citation.cfm?id=231986.231989},
volume = {24},
year = {1996}
}
@article{Brodersen2014,
author = {Brodersen, By Kay H and Gallusser, Fabian and Koehler, Jim and Remy, Nicolas and Scott, Steven L.},
file = {:E$\backslash$:/Dropbox/phd/bsts/INFERRING CAUSAL IMPACT USING BSTS.pdf:pdf},
journal = {Annals of Applied Statistics},
title = {{Inferring causal impact using Bayesian structural time-series models}},
year = {2014}
}
@article{Brodersen2013,
abstract = {An important problem in econometrics and marketing is to infer the causal impact that a designed market intervention has exerted on an outcome metric over time. In order to allocate a given budget optimally, for example, an advertiser must determine the incremental contributions that different advertising campaigns have made to web searches, product installs, or sales. This paper proposes to infer causal impact on the basis of a diffusion-regression state-space model that predicts the counterfactual market response that would have occurred had no intervention taken place. In con- trast to classical difference-in-differences schemes, state-space models make it possible to (i) infer the temporal evolution of attributable impact, (ii) incorporate empirical priors on the parameters in a fully Bayesian treatment, and (iii) flexibly accommodate multiple sources of variation, including the time-varying influence of contemporane- ous covariates, i.e., synthetic controls. Using a Markov chain Monte Carlo algorithm for posterior inference, we illustrate the statistical properties of our approach on synthetic data. We then demonstrate its practical utility by evaluating the effect of an online advertising campaign on search-related site visits. We discuss the strengths and limitations of our approach in improving the accuracy of causal at- tribution, power analyses, and principled budget allocation.},
author = {Brodersen, Kh and Gallusser, Fabian and Koehler, Jim},
keywords = {causal inference, counterfactual, synthetic contro},
number = {June},
pages = {1--32},
title = {{Inferring Causal Impact Using Bayesian Structural Time-Series Models}},
url = {http://192.249.122.146/sites/default/files/Inferring causal impact using Bayesian structural time-series models.pdf},
year = {2013}
}
@article{Buchen2011a,
abstract = {This paper evaluates the forecast performance of boosting in comparison to the forecast combination schemes and dynamic factor models presented in Stock and Watson (2006). We find that boosting is a serious competitor for forecasting US industrial production.},
author = {Buchen, Teresa and Wohlrabe, Klaus},
doi = {10.1016/j.econlet.2011.05.040},
file = {:C$\backslash$:/Users/snowdj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Buchen, Wohlrabe - 2011 - Forecasting with many predictors Is boosting a viable alternative.pdf:pdf},
issn = {01651765},
journal = {Economics Letters},
keywords = {Boosting,Forecasting,Large datasets},
month = oct,
number = {1},
pages = {16--18},
title = {{Forecasting with many predictors: Is boosting a viable alternative?}},
url = {http://www.sciencedirect.com/science/article/pii/S0165176511002175},
volume = {113},
year = {2011}
}
@book{Cameron2005,
author = {Cameron, A Colin and Trivedi, Pravin K},
booktitle = {Analysis},
doi = {10.1016/S0304-4076(00)00050-6},
institution = {Center for Biomass Resource Utilization, College of Engineering, China Agricultural University (East Campus), 17 Qing-Hua-Dong-Lu, Hai-Dian District, Beijing 100083, PR China.},
isbn = {9780521848053},
issn = {03044076},
number = {1},
pages = {1056},
pmid = {20074927},
publisher = {Cambridge University Press},
series = {Cambridge Books},
title = {{Microeconometrics: Methods and Applications}},
url = {http://books.google.com/books?hl=en\&amp;lr=\&amp;id=Zf0gCwxC9ocC\&amp;oi=fnd\&amp;pg=PR15\&amp;dq=Microeconometrics+Methods+and+Applications\&amp;ots=CY25lK3EqT\&amp;sig=EU9ni1rMTBFUc25MBWG3nuKLol0},
volume = {100},
year = {2005}
}
@article{Carriero2012,
abstract = {This paper develops a method for producing current-quarter forecasts of GDP growth with a (possibly large) range of available within-the-quarter monthly observations of economic indicators, such as employment and industrial production, and financial indicators, such as stock prices and interest rates. In light of existing evidence of time variation in the variances of shocks to GDP, we consider versions of the model with both constant variances and stochastic volatility. We also evaluate models with either constant or time-varying regression coefficients. We use Bayesian methods to estimate the model, in order to facilitate providing shrinkage on the (possibly large) set of model parameters and conveniently generate predictive densities. We provide results on the accuracy of nowcasts of real-time GDP growth in the U.S. from 1985 through 2011. In terms of point forecasts, our proposal is comparable to alternative econometric methods and survey forecasts. In addition, it provides reliable density forecasts, for which the stochastic volatility specification is quite useful, while parameter time-variation does not seem to matter.},
author = {Carriero, Andrea and Clark, Todd E and Marcellino, Massimiliano},
file = {:E$\backslash$:/Dropbox/phd/bsts/Carriero\_et\_al-2015-Journal\_of\_the\_Royal\_Statistical\_Society\_\_Series\_A\_(Statistics\_in\_Society).pdf:pdf},
journal = {Journal of the Royal Statistical Society: Series A (Statistics in Society)},
keywords = {an associate,bayesian methods,brent meyer,c22,c53,cation,classi,domenico giannone,e,e37,editor,editor arnaud chevalier,forecasting,helpful suggestions from the,j,knut are aastveit,l,marta banbura,mixed frequency models,prediction,the authors gratefully acknowledge,two anonymous referees},
title = {{Real-Time Nowcasting With a Bayesian Mixed Frequency Model With Stochastic Volatility}},
year = {2015}
}
@article{Carriero2011,
author = {Carriero, Andrea and Kapetanios, George and Marcellino, Massimiliano},
doi = {10.1002/jae.1150},
file = {:C$\backslash$:/Users/snowdj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Carriero, Kapetanios, Marcellino - 2010 - Forecasting large datasets with Bayesian reduced rank multivariate models.pdf:pdf},
issn = {08837252},
journal = {Journal of Applied Econometrics},
month = aug,
number = {5},
pages = {735--761},
title = {{Forecasting large datasets with Bayesian reduced rank multivariate models}},
url = {http://doi.wiley.com/10.1002/jae.1150},
volume = {26},
year = {2011}
}
@article{Castle2013,
abstract = {We consider forecasting with factors, variables and both, modeling in-sample using Autometrics so all principal components and variables can be included jointly, while tackling multiple breaks by impulse-indicator saturation. A forecast-error taxonomy for factor models highlights the impacts of location shifts on forecast-error biases. Forecasting US GDP over 1-, 4- and 8-step horizons using the dataset from Stock and Watson (2009) updated to 2011:2 shows factor models are more useful for nowcasting or short-term forecasting, but their relative performance declines as the forecast horizon increases. Forecasts for GDP levels highlight the need for robust strategies, such as intercept corrections or differencing, when location shifts occur as in the recent financial crisis.},
author = {Castle, Jennifer L. and Clements, Michael P and Hendry, David},
doi = {10.1016/j.jeconom.2013.04.015},
file = {:E$\backslash$:/Dropbox/phd/MIDAS/factors/Forecasting by factors, by variables, by both or neither.pdf:pdf},
issn = {03044076},
journal = {Journal of Econometrics},
keywords = {Autometrics,C22,C51,Factor models,Forecasting,Impulse-indicator saturation,Model selection},
month = dec,
number = {2},
pages = {305--319},
title = {{Forecasting by factors, by variables, by both or neither?}},
url = {http://www.sciencedirect.com/science/article/pii/S0304407613000924},
volume = {177},
year = {2013}
}
@article{Castle2015,
abstract = {a b s t r a c t We investigate alternative robust approaches to forecasting, using a new class of robust devices, contrasted with equilibrium-correction models. Their forecasting properties are derived facing a range of likely empirical problems at the forecast origin, including mea-surement errors, impulses, omitted variables, unanticipated location shifts and incorrectly included variables that experience a shift. We derive the resulting forecast biases and error variances, and indicate when the methods are likely to perform well. The robust methods are applied to forecasting US GDP using autoregressive models, and also to autoregressive models with factors extracted from a large dataset of macroeconomic variables. We con-sider forecasting performance over the Great Recession, and over an earlier more quiescent period.},
author = {Castle, Jennifer L and Clements, Michael P and Hendry, David},
doi = {10.1016/j.ijforecast.2014.11.002},
file = {:C$\backslash$:/Users/snowdj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Castle, Clements, Hendry - 2015 - Robust approaches to forecasting.pdf:pdf},
journal = {International Journal of Forecasting},
keywords = {Factor models GDP forecasts,Forecast biases,Location shifts,Smoothed forecasting devices},
pages = {99--112},
title = {{Robust approaches to forecasting}},
volume = {31},
year = {2015}
}
@article{Castle2012,
abstract = {We consider forecasting with factors, variables and both, modeling in-sample using Autometrics so all principal components and variables can be included jointly, while tackling multiple breaks by impulse-indicator saturation.� A forecast-error taxonomy for factor models highlights the impacts of location shifts on forecast-error biases.� Forecasting US GDP over 1-, 4- and 8-step horizons using the dataset from Stock and Watson (2009) updated to 2011:2 shows factor models are more useful for nowcasting or short-term forecasting, but their relative performance declines as the forecast horizon increases.� Forecasts for GDP levels highlight the need for robust strategies such as intercept corrections or differencing when location shifts occur, as in the recent financial crisis.},
author = {Castle, Jennifer and Hendry, David},
journal = {Economics Series Working Papers},
keywords = {Autometrics,Factor models,Forecasting,Impulse-indicator saturation,Model selection},
month = apr,
publisher = {University of Oxford, Department of Economics},
title = {{Forecasting by factors, by variables, or both?}},
url = {http://ideas.repec.org/p/oxf/wpaper/600.html},
year = {2012}
}
@article{Castle2013a,
abstract = {We consider the reasons for nowcasting, how nowcasts can be achieved, and the use and timing of information.� The existence of contemporaneous data such as surveys is a major difference from forecasting, but many of the recent lessons about forecasting remain relevant.� Given the extensive disaggregation over variables underlying flash estimates of aggregates, we show that automatic model selection can play a valuable role, especially when location shifts would otherwise induce nowcast failure.� Thus, we address nowcasting when location shifts occur, probably with measurement error.� We describe impulse-indicator saturation as a potential solution to such shifts, noting its relation to intercept corrections and to robust methods to avoid systematic nowcast failure.� We propose a nowcasting strategy, building models of all disaggregate series by automatic methods, forecasting all variables before the end of each period, testing for shifts as available measures arrive, and adjusting forecasts of cognate missing series if substantive discrepancies are found.� An alternative is switching to robust forecasts when breaks are detected.� We apply a variant of this strategy to nowcast UK GDP growth, seeking pseudo real-time data availability.},
author = {Castle, Jennifer and Hendry, David},
journal = {Economics Series Working Papers},
keywords = {Autometrics,Contemporaneous information,Forecasting,Impulse-indicator saturation,Location shifts,Nowcasting},
month = sep,
publisher = {University of Oxford, Department of Economics},
title = {{Forecasting and Nowcasting Macroeconomic Variables: A Methodological Overview}},
url = {http://ideas.repec.org/p/oxf/wpaper/674.html},
year = {2013}
}
@article{Castle2014,
abstract = {We investigate alternative robust approaches to forecasting, using a new class of robust devices, contrasted with equilibrium correction models.� Their forecasting properties are derived facing a range of likely empirical problems at the forecast origin, including measurement errors, implulses, omitted variables, unanticipated location shifts and incorrectly included variables that experience a shift.� We derive the resulting forecast biases and error variances, and indicate when the methods are likely to perform well.� The robust methods are applied to forecasting US GDP using autoregressive models, and also to autoregressive models with factors extracted from a large dataset of macroeconomic variables.� We consider forecasting performance over the Great Recession, and over an earlier more quiescent period.},
author = {Castle, Jennifer and Hendry, David and Clements, Michael P},
journal = {Economics Series Working Papers},
keywords = {Factor models,GDP forecasts,Location shifts,Robust forecasts,Smoothed Forecasting devices},
month = jan,
publisher = {University of Oxford, Department of Economics},
title = {{Robust Approaches to Forecasting}},
url = {http://ideas.repec.org/p/oxf/wpaper/697.html},
year = {2014}
}
@article{Chauvet2014,
author = {Chauvet, Marcelle and Thomas, G and Hecq, Alain},
file = {:E$\backslash$:/Dropbox/phd/MIDAS/A Mixed-Frequency VAR Approach.pdf:pdf},
pages = {1--34},
title = {{Realized Volatility and Business Cycle Fluctuations : A Mixed-Frequency VAR Approach}},
year = {2014}
}
@article{An2011,
author = {Chen, Yu-chin and Tsay, Wen-Jen},
file = {:E$\backslash$:/Dropbox/phd/MIDAS/ADL-ssrn.pdf:pdf},
keywords = {2011,and jonathan wright for,autoregressive distributed lag,c22,c53,cations,charles nelson,comments and discussions,commodity prices,department of,f31,f47,february,first draft,forecasting,jel classi,mixed frequency data,q02,the research is conducted,useful,visiting scholar in the,we thank eric ghysels,when tsay is a},
title = {{Forecasting Commodity Prices with Mixed\# Frequency Data: An OLS Based Generalized ADL Approach}},
url = {http://www.econ.sinica.edu.tw/upload/file/(20111020)(1).pdf},
year = {2011}
}
@article{Cheng2013,
author = {Cheng, Xu and Schorfheide, Frank},
keywords = {consistent model selection,factor model,great recession,high-dimensional,large data sets,lasso,model,shrinkage estimation,structural break},
title = {{Shrinkage Estimation of High-Dimensional Factor Models with Structural Instabilities}},
year = {2013}
}
@article{Chipman2001,
author = {Chipman, Hugh and George, Edward I. and McCulloch, Robert E.},
file = {:C$\backslash$:/Users/snowdj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chipman, George, McCulloch - 2001 - The Practical Implementation of Bayesian Model Selection.pdf:pdf},
isbn = {0-940600-52-8},
journal = {Lecture Notes-Monograph Series},
language = {EN},
pages = {65--134},
publisher = {Institute of Mathematical Statistics},
title = {{The Practical Implementation of Bayesian Model Selection}},
url = {http://projecteuclid.org/euclid.lnms/1215540964},
year = {2001}
}
@article{Clements2014,
author = {Clements, Michael P},
number = {May},
title = {{Real-Time Factor Model Forecasting and the Effects of Instability}},
year = {2014}
}
@article{Clements2008a,
abstract = {Many macroeconomic series, such as U.S. real output growth, are sampled quarterly, although potentially useful predictors are often observed at a higher frequency. We look at whether a mixed data-frequency sampling (MIDAS) approach can improve forecasts of output growth. The MIDAS specification used in the comparison uses a novel way of including an autoregressive term. We find that the use of monthly data on the current quarter leads to significant improvement in forecasting current and next quarter output growth, and that MIDAS is an effective way to exploit monthly data compared with alternative methods.},
author = {Clements, Michael P and Galv\~{a}o, Ana Beatriz},
doi = {10.1198/073500108000000015},
isbn = {4424765229},
issn = {0735-0015},
keywords = {H Social Sciences,HC Economic History and Conditions,QA Mathematics},
number = {May 2015},
pages = {37--41},
title = {{Macroeconomic Forecasting With Mixed-Frequency Data: Forecasting Output Growth in the United States}},
url = {http://dx.doi.org/10.1198/073500108000000015},
year = {2008}
}
@article{Clements2009,
author = {Clements, Michael P and Galv\~{a}o, Ana Beatriz},
doi = {10.1002/jae.1075},
issn = {08837252},
journal = {Journal of Applied Econometrics},
month = nov,
number = {7},
pages = {1187--1206},
title = {{Forecasting US output growth using leading indicators: an appraisal using MIDAS models}},
url = {http://doi.wiley.com/10.1002/jae.1075},
volume = {24},
year = {2009}
}
@article{Clements2008,
abstract = {Many macroeconomic series, such as U.S. real output growth, are sampled quarterly, although potentially useful predictors are often observed at a higher frequency. We look at whether a mixed data-frequency sampling (MIDAS) approach can improve forecasts of output growth. The MIDAS specification used in the comparison uses a novel way of including an autoregressive term. We find that the use of monthly data on the current quarter leads to significant improvement in forecasting current and next quarter output growth, and that MIDAS is an effective way to exploit monthly data compared with alternative methods.},
author = {Clements, Michael P and Galv\~{a}o, Ana Beatriz},
doi = {10.1198/073500108000000015},
issn = {0735-0015},
journal = {Journal of Business \& Economic Statistics},
keywords = {Forecasting,Mixed-frequency data,U.S. output growth},
month = oct,
number = {4},
pages = {546--554},
publisher = {Taylor \& Francis},
title = {{Macroeconomic Forecasting With Mixed-Frequency Data}},
url = {http://www.tandfonline.com/doi/abs/10.1198/073500108000000015},
volume = {26},
year = {2008}
}
@article{Clyde1999,
author = {Clyde, Merlise a.},
journal = {Bayesian Statistics 6},
keywords = {generalized linear models,monte carlo,reversible jump markov chain},
pages = {157--185},
title = {{Bayesian Model Averaging and Model Search Strategies}},
year = {1999}
}
@article{Commandeur2007,
author = {Commandeur, Jacques J.F and Koopman, Siem Jan},
file = {:C$\backslash$:/Users/snowdj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Commandeur, Koopman - 2007 - An Introduction to State Space Time Series Analysis.pdf:pdf},
isbn = {978-0-19-922887-4},
pages = {189},
title = {{An Introduction to State Space Time Series Analysis}},
year = {2007}
}
@article{DeMol2008a,
abstract = {This paper considers Bayesian regression with normal and double-exponential priors as forecasting methods based on large panels of time series. We show that, empirically, these forecasts are highly correlated with principal component forecasts and that they perform equally well for a wide range of prior choices. Moreover, we study conditions for consistency of the forecast based on Bayesian regression as the cross-section and the sample size become large. This analysis serves as a guide to establish a criterion for setting the amount of shrinkage in a large cross-section. ?? 2008 Elsevier B.V. All rights reserved.},
author = {{De Mol}, Christine and Giannone, Domenico and Reichlin, Lucrezia},
doi = {10.1016/j.jeconom.2008.08.011},
file = {:C$\backslash$:/Users/snowdj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/De Mol, Giannone, Reichlin - 2008 - Forecasting using a large number of predictors Is Bayesian shrinkage a valid alternative to princ(2).pdf:pdf},
issn = {03044076},
journal = {Journal of Econometrics},
keywords = {Bayesian VAR,Bayesian shrinkage,Large cross-sections,Lasso regression,Principal components,Ridge regression},
title = {{Forecasting using a large number of predictors: Is Bayesian shrinkage a valid alternative to principal components?}},
year = {2008}
}
@article{DeMol2006,
abstract = {This paper considers Bayesian regression with normal and doubleexponential priors as forecasting methods based on large panels of time series. We show that, empirically, these forecasts are highly correlated with principal component forecasts and that they perform equally well for a wide range of prior choices. Moreover, we study the asymptotic properties of the Bayesian regression under Gaussian prior under the assumption that data are quasi collinear to establish a criterion for setting parameters in a large cross-section.},
author = {{De Mol}, Christine and Giannone, Domenico and Reichlin, Lucrezia},
file = {:C$\backslash$:/Users/snowdj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/De Mol, Giannone, Reichlin - 2006 - Forecasting using a large number of predictors is Bayesian regression a valid alternative to princip.pdf:pdf},
journal = {Discussion Paper Series 1: Economic Studies},
keywords = {Bayesian VAR,Lasso regression,large cross-sections,principal components,ridge regression},
publisher = {Deutsche Bundesbank, Research Centre},
title = {{Forecasting using a large number of predictors: is Bayesian regression a valid alternative to principal components?}},
url = {http://ideas.repec.org/p/zbw/bubdp1/5040.html},
year = {2006}
}
@article{Doz2006,
abstract = {This paper considers quasi-maximum likelihood estimations of a dynamic ap- proximate factor model when the panel of time series is large. Maximum likelihood is analyzed under different sources of misspecification: omitted serial correlation of the observations and cross-sectional correlation of the idiosyncratic components. It is shown that the effects ofmisspecification on the estimation of the common factors is negligible for large sample size (T) and the cross-sectional dimension (n). The estimator is feasible when n is large and easily implementable using the Kalman smoother and theEMalgorithm as in traditional factor analysis. Simulation results illustrate what are the empirical conditions in which we can expect improvement with respect to simple principle components considered by Bai (2003), Bai and Ng (2002), Forni, Hallin, Lippi, and Reichlin (2000, 2005b), Stock and Watson (2002a,b).},
author = {Doz, Catherine and Giannone, Domenico and Reichlin, Lucrezia},
keywords = {Factor Model,Quasi Maximum Likelihood.,large cross-sections},
number = {674},
title = {{A quasi maximum likelihood approach for large approximate dynamic factor models}},
year = {2006}
}
@article{Doz2011,
abstract = {This paper shows consistency of a two-step estimation of the factors in a dynamic approximate factor model when the panel of time series is large (n large). In the first step, the parameters of the model are estimated from an OLS on principal components. In the second step, the factors are estimated via the Kalman smoother. The analysis develops the theory for the estimator considered in Giannone et al. (2004) and Giannone et al. (2008) and for the many empirical papers using this framework for nowcasting.},
author = {Doz, Catherine and Giannone, Domenico and Reichlin, Lucrezia},
doi = {10.1016/j.jeconom.2011.02.012},
issn = {03044076},
journal = {Journal of Econometrics},
keywords = {C32,C33,C51,Factor models,Kalman filter,Large cross-sections,Principal components},
month = sep,
number = {1},
pages = {188--205},
title = {{A two-step estimator for large approximate dynamic factor models based on Kalman filtering}},
url = {http://www.sciencedirect.com/science/article/pii/S030440761100039X},
volume = {164},
year = {2011}
}
@book{Duarte2014,
author = {Duarte, Cl\'{a}udia},
file = {:E$\backslash$:/Dropbox/phd/MIDAS/AR-MIDAS/Autoregressive augmentation of MIDAS regressions.pdf:pdf},
title = {{AUTOREGRESSIVE AUGMENTATION OF}},
year = {2014}
}
@book{Durbin2012,
abstract = {This new edition updates Durbin \& Koopman's important text on the state space approach to time series analysis. The distinguishing feature of state space time series models is that observations are regarded as made up of distinct components such as trend, seasonal, regression elements and disturbance terms, each of which is modelled separately. The techniques that emerge from this approach are very flexible and are capable of handling a much wider range of problems than the main analytical system currently in use for time series analysis, the Box-Jenkins ARIMA system. Additions to this second edition include the filtering of nonlinear and non-Gaussian series. Part I of the book obtains the mean and variance of the state, of a variable intended to measure the effect of an interaction and of regression coefficients, in terms of the observations. Part II extends the treatment to nonlinear and non-normal models. For these, analytical solutions are not available so methods are based on simulation.},
author = {Durbin, James and Koopman, Siem Jan Sj},
file = {:C$\backslash$:/Users/snowdj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Durbin, Koopman - 2012 - Time Series Analysis by State Space Methods Second Edition.pdf:pdf},
isbn = {019964117X},
pages = {346},
publisher = {Oxford University Press},
title = {{Time series analysis by state space methods}},
url = {http://books.google.com/books?hl=en\&lr=\&id=fOq39Zh0olQC\&oi=fnd\&pg=PP2\&dq=time+series+analysis+by+state+space+methods\&ots=o95cm4AVyi\&sig=2jtVmFvgu0ZJ8d0UOE8skg0CBr4$\backslash$nhttp://books.google.com/books?hl=en\&lr=\&id=fOq39Zh0olQC\&oi=fnd\&pg=PP2\&dq=time+series+analy},
year = {2012}
}
@article{Economics2006,
author = {Economics, Development and Nakstad, Yoon Shin},
file = {:E$\backslash$:/Dropbox/phd/statespace/Thesis\_Structural\_time\_series\_model\_FINALFINAL.pdf:pdf},
number = {August},
title = {{Structural Time Series Models : theory and application Department of Economics}},
year = {2006}
}
@article{Eiswerth2007,
author = {Eiswerth, Mark E. and van Kooten, G. Cornelis},
doi = {10.1111/j.1744-7976.2007.00104.x},
issn = {0008-3976},
journal = {Canadian Journal of Agricultural Economics/Revue canadienne d'agroeconomie},
month = dec,
number = {4},
pages = {485--498},
title = {{Dynamic Programming and Learning Models for Management of a Nonnative Species}},
url = {http://doi.wiley.com/10.1111/j.1744-7976.2007.00104.x},
volume = {55},
year = {2007}
}
@article{Eraker2008,
abstract = {Economic data can be collected at a variety of frequencies. Typically, estimation is done using the frequency of the coarsest data. This paper discusses how to combine data of different frequencies in estimating Vector Autoregressions (VAR’s). The method is based on Bayesian Gibbs sampling using a missing data formulation for coarsely observed data. Our approach has the primary advantage that it increases the accuracy of parameter estimates relative to estimates obtained from coarse data. We demonstrate this through an example where we estimate a model for economic growth based on quarterly observations of GDP, monthly industrial production, and yield curve data. Estimates of the posterior standard deviations are uniformly lower for the BMF estimator. Experiments with artificially simulated data further docu- ments the efficiency gains.},
author = {Eraker, Bj\o rn and Wai, Ching and Chiu, Jeremy and Foerster, Andrew and Kim, Tae Bong},
file = {:C$\backslash$:/Users/snowdj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Eraker et al. - 2008 - Bayesian Mixed Frequency VAR's.pdf:pdf},
pages = {1--21},
title = {{Bayesian Mixed Frequency VAR's}},
year = {2008}
}
@article{Ferrara2013,
author = {Ferrara, Laurent and Marsilli, Cl\'{e}ment},
keywords = {and do not reflect,are solely,banque de france,bayesian variable selection,forecasting,gong cheng for his,helpful comments,lasso,midas,thank arnaud dufays,the views expressed herein,the views of the,those of the authors,we would like to},
title = {{Variable selection with mixed frequencies : an assessment based on macroeconomic forecasting}},
year = {2013}
}
@article{Ferrara2014,
abstract = {The Great Recession endured by the main industrialized countries during the period 2008-2009, in the wake of the financial and banking crisis, has pointed out the major role of the financial sector on macroeconomic fluctuations. In this respect, many researchers have started to reconsider the linkages between financial and macroeconomic areas. In this paper, we evaluate the leading role of the daily volatility of two major financial variables, namely commodity and stock prices, in their ability to anticipate the output growth. For this purpose, we propose an extended MIDAS model that allows the forecasting of the quarterly output growth rate using exogenous variables sampled at various higher frequencies. Empirical results on three industrialized countries (US, France, and UK) show that mixing daily financial volatilities and monthly industrial production is useful at the time of predicting gross domestic product growth over the Great Recession period. © 2013 Elsevier B.V.},
author = {Ferrara, Laurent and Marsilli, Cl\'{e}ment and Ortega, Juan Pablo},
doi = {10.1016/j.econmod.2013.08.042},
issn = {02649993},
journal = {Economic Modelling},
keywords = {Financial variables,Forecasting,Great Recession,MIDAS approach,Volatility},
pages = {44--50},
publisher = {Elsevier B.V.},
title = {{Forecasting growth during the Great Recession: Is financial volatility the missing ingredient?}},
url = {http://dx.doi.org/10.1016/j.econmod.2013.08.042},
volume = {36},
year = {2014}
}
@article{Foroni2014b,
author = {Foroni, Claudia and Gu, Pierre and Marcellino, Massimiliano},
keywords = {fore-,markov-switching,midas,mixed-frequency var,nowcasting},
title = {{Markov-Switching Mixed-Frequency VAR Models ∗}},
year = {2014}
}
@article{Foroni2015,
author = {Foroni, Claudia and Marcellino, Massimiliano},
file = {:E$\backslash$:/Dropbox/phd/MIDAS/UMIDAS/Foroni\_et\_al-2015-Journal\_of\_the\_Royal\_Statistical\_Society\_\_Series\_A\_(Statistics\_in\_Society).pdf:pdf},
journal = {Journal of Royal Statistical. Statistics in soceiety. Seires A},
keywords = {distributed lag polynomals,mixed data sampling,nowcasting,time aggregation},
number = {Part 1},
pages = {57--82},
title = {{Unrestricted mixed data sampling ( MIDAS ): MIDAS regressions with unrestricted lag polynomials}},
volume = {178},
year = {2015}
}
@article{Foroni2014a,
abstract = {In this paper, we focus on the different methods which have been proposed in the literature to date for dealing with mixed-frequency and ragged-edge datasets: bridge equations, mixed-data sampling (MIDAS), and mixed-frequency VAR (MF-VAR) models. We discuss their performances for nowcasting the quarterly growth rate of the Euro area GDP and its components, using a very large set of monthly indicators. We investigate the behaviors of single indicator models, forecast combinations and factor models, in a pseudo real-time framework. MIDAS with an AR component performs quite well, and outperforms MF-VAR at most horizons. Bridge equations perform well overall. Forecast pooling is superior to most of the single indicator models overall. Pooling information using factor models gives even better results. The best results are obtained for the components for which more economically related monthly indicators are available. Nowcasts of GDP components can then be combined to obtain nowcasts for the total GDP growth. ?? 2013 International Institute of Forecasters.},
author = {Foroni, Claudia and Marcellino, Massimiliano},
doi = {10.1016/j.ijforecast.2013.01.010},
file = {:C$\backslash$:/Users/snowdj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Foroni, Marcellino - 2014 - A comparison of mixed frequency approaches for nowcasting Euro area macroeconomic aggregates.pdf:pdf},
issn = {01692070},
journal = {International Journal of Forecasting},
keywords = {Bridge models,Factor models,MIDAS,Mixed-frequency VAR,Mixed-frequency data,Nowcasting},
title = {{A comparison of mixed frequency approaches for nowcasting Euro area macroeconomic aggregates}},
year = {2014}
}
@article{Foroni2013,
abstract = {The development of models for variables sampled at di¤erent frequencies has attracted substantial interest in the recent econometric literature. In this paper we provide an overview of the most common techniques, including bridge equa- tions, MIxed DAta Sampling (MIDAS) models, mixed frequency VARs, and mixed frequency factor models. We also consider alternative techniques for handling the ragged edge of the data, due to asynchronous publication. Finally, we survey the main empirical applications based on alternative mixed frequency models. J.E.L.},
author = {Foroni, Claudia and Marcellino, Massimiliano},
doi = {10.2139/ssrn.2268912},
file = {:E$\backslash$:/Dropbox/phd/MIDAS/A survey of econometric methods for mixed-frequency data.pdf:pdf},
isbn = {9788275537230},
issn = {1556-5068},
journal = {Available at SSRN 2268912},
keywords = {Claudia Foroni,Massimiliano Marcellino},
title = {{A Survey of Econometric Methods for Mixed-Frequency Data}},
year = {2013}
}
@article{Frale2011,
author = {Frale, Cecilia and Marcellino, Massimiliano and Mazzi, Gian Luigi and Proietti, Tommaso},
doi = {10.1111/j.1467-985X.2010.00675.x},
issn = {09641998},
journal = {Journal of the Royal Statistical Society. Series A: Statistics in Society},
keywords = {Chain linking,Dynamic factor models,Kalman filter and smoother,Multivariate state space models,Temporal disaggregation},
month = apr,
number = {2},
pages = {439--470},
title = {{EUROMIND: A monthly indicator of the euro area economic conditions}},
url = {http://doi.wiley.com/10.1111/j.1467-985X.2010.00675.x},
volume = {174},
year = {2011}
}
@article{Galbraith2014,
author = {Galbraith, John W and Tkacz, Greg},
title = {{Nowcasting GDP : Electronic Payments , Data Vintages and the Timing of Data Releases}},
year = {2014}
}
@article{Galvao2013,
abstract = {When assessing the predictive power of financial variables for economic activity, researchers usually aggregate higher-frequency data before estimating a forecasting model that assumes the relationship between the financial variable and the dependent variable to be linear. This paper proposes a model called smooth transition mixed data sampling (STMIDAS) regression, which relaxes both of these assumptions. Simulation exercises indicate that the improvements in forecasting accuracy from the use of mixed data sampling are larger in nonlinear than in linear specifications. When forecasting output growth with financial variables in real time, statistically significant improvements over a linear regression are more likely to arise from forecasting with STMIDAS than with MIDAS regressions.},
author = {Galv\~{a}o, Ana Beatriz},
doi = {10.1016/j.ijforecast.2012.10.006},
file = {:C$\backslash$:/Users/snowdj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Galv\~{a}o - 2013 - Changes in predictive ability with mixed frequency data.pdf:pdf},
issn = {01692070},
journal = {International Journal of Forecasting},
keywords = {Economic activity,Financial indicators,MIDAS,Predictive ability,Smooth transition},
month = jul,
number = {3},
pages = {395--410},
title = {{Changes in predictive ability with mixed frequency data}},
url = {http://www.sciencedirect.com/science/article/pii/S0169207012001689},
volume = {29},
year = {2013}
}
@book{Gelman2013,
abstract = {Now in its third edition, this classic book is widely considered the leading text on Bayesian methods, lauded for its accessible, practical approach to analyzing data and solving research problems. Bayesian Data Analysis, Third Edition continues to take an applied approach to analysis using up-to-date Bayesian methods. The authors—all leaders in the statistics community—introduce basic concepts from a data-analytic perspective before presenting advanced methods. Throughout the text, numerous worked examples drawn from real applications and research emphasize the use of Bayesian inference in practice. New to the Third Edition  Four new chapters on nonparametric modeling Coverage of weakly informative priors and boundary-avoiding priors Updated discussion of cross-validation and predictive information criteria Improved convergence monitoring and effective sample size calculations for iterative simulation Presentations of Hamiltonian Monte Carlo, variational Bayes, and expectation propagation New and revised software code  The book can be used in three different ways. For undergraduate students, it introduces Bayesian inference starting from first principles. For graduate students, the text presents effective current approaches to Bayesian modeling and computation in statistics and related fields. For researchers, it provides an assortment of Bayesian methods in applied statistics. Additional materials, including data sets used in the examples, solutions to selected exercises, and software instructions, are available on the book’s web page.},
author = {Gelman, Andrew and Carlin, John B. and Stern, Hal S. and Dunson, David B. and Vehtari, Aki and Rubin, Donald B.},
isbn = {1439840954},
pages = {675},
publisher = {CRC Press},
title = {{Bayesian Data Analysis, Third Edition}},
url = {https://books.google.ca/books/about/Bayesian\_Data\_Analysis\_Third\_Edition.html?id=ZXL6AQAAQBAJ\&pgis=1},
volume = {1},
year = {2013}
}
@book{Gelman2003,
author = {Gelman, Andrew and Carlin, John and Stern, Hal and Rubin, Donald},
isbn = {158488388X},
publisher = {Chapman and Hall/CRC},
title = {{Bayesian Data Analysis, Second Edition (Chapman \& Hall/CRC Texts in Statistical Science)}},
year = {2003}
}
@article{George1997,
abstract = {This paper describes and compares various hierarchical mixture prior formulations of variable selection uncertainty in normal linear regression models. These include the nonconjugate SSVS formulation of George andMcCulloch (1993), as well as conjugate formulations which allow for analytical simplification. Hyperpa- rameter settings which base selection on practical significance, and the implications of using mixtures with point priors are discussed. Computational methods for pos- terior evaluation and exploration are considered. Rapid updating methods are seen to provide feasible methods for exhaustive evaluation using Gray Code sequencing in moderately sized problems, and fast Markov Chain Monte Carlo exploration in large problems. Estimation of normalization constants is seen to provide improved posterior estimates of individual model probabilities and the total visited probabil- ity. Various procedures are illustrated on simulated sample problems and on a real problem concerning the construction of financial index tracking portfolios.},
author = {George, Edward I and Mcculloch, Robert E},
doi = {10.1.1.211.4871},
file = {:C$\backslash$:/Users/snowdj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/George, Mcculloch - 1997 - Approaches for bayesian variable selection.pdf:pdf},
isbn = {1017-0405},
issn = {10170405},
journal = {Statistica Sinica},
keywords = {and phrases,cal models,conjugate prior,gibbs sampling,gray code,hierarchi-,markov chain monte carlo,metropolis-hastings algorithms,mixtures,normal,normalization constant,regression,simulation},
pages = {339--373},
title = {{Approaches for bayesian variable selection}},
volume = {7},
year = {1997}
}
@article{Ghysels2012,
abstract = {Many time series are sampled at different frequencies. When we study co-movements between such series we usually analyze the joint process sampled at a common low frequency. This has consequences in terms of potentially mis-specifying the co- movements and hence the analysis of impulse response functions - a commonly used tool for economic policy analysis. We introduce a class of mixed frequency VAR models that allows us to measure the impact of high frequency data on low frequency and vice versa. Our approach does not rely on latent processes/shocks representations. As a consequence, the mixed frequency VAR is an alternative to commonly used state space models for mixed frequency data. State space models are parameter-driven whereas mixed frequency VAR models are observation-driven models as they are formulated exclusively in terms of observable data and do not involve latent processes as well as shocks and thus avoid the need to formulate measurement equations, filtering etc. We also propose various parsimonious parameterizations, in part inspired by recent work on MIDAS regressions. We also explicitly characterize the mis-specification of a traditional common low frequency VAR and its implied mis-specified impulse response functions. The class of mixed frequency VAR models can also characterize the timing of information releases for a mixture of sampling frequencies and the real-time updating of predictions caused by the flow of high frequency information. Various estimation procedures for mixed frequency VAR models are also proposed, both classical and Bayesian. Numerical and empirical examples quantify the consequences of ignoring mixed frequency data.},
author = {Ghysels, Eric},
journal = {University of North Carolina Working Paper},
number = {July 2011},
pages = {1--52},
title = {{Macroeconomics and the Reality of Mixed Frequency Data}},
year = {2012}
}
@article{Ghysels2012a,
author = {Ghysels, Eric},
file = {:E$\backslash$:/Dropbox/phd/MIDAS/VAR/Mixed Frequency Vector Autoregressive Models.pdf:pdf},
journal = {mimeo},
title = {{Mixed Frequency Vector Autoregressive Models}},
year = {2012}
}
@article{Ghysels2004,
author = {Ghysels, Eric and Santa-Clara, Pedro and Valkanov, Rossen},
journal = {Finance},
month = jun,
title = {{The MIDAS Touch: Mixed Data Sampling Regression Models}},
url = {https://escholarship.org/uc/item/9mf223rs\#page-3},
year = {2004}
}
@article{Ghysels2007,
abstract = {We explore mixed data sampling (henceforth MIDAS) regression models. The regressions involve time series data sampled at different frequencies. Volatility and related processes are our prime focus, though the regression method has wider applications in macroeconomics and finance, among other areas. The regressions combine recent developments regarding estimation of volatility and a not-so-recent literature on distributed lag models. We study various lag structures to parameterize parsimoniously the regressions and relate them to existing models. We also propose several new extensions of the MIDAS framework. The paper concludes with an empirical section where we provide further evidence and new results on the risk–return trade-off. We also report empirical evidence on microstructure noise and volatility forecasting.},
author = {Ghysels, Eric and Sinko, Arthur and Valkanov, Rossen},
doi = {10.1080/07474930600972467},
file = {:E$\backslash$:/Dropbox/phd/MIDAS/MIDAS REGRESSIONS FURTHER RESULTS AND NEW DIRECTIONS.pdf:pdf},
issn = {0747-4938},
journal = {Econometric Reviews},
keywords = {C22,C53,MIDAS,Microstructure noise,Nonlinear MIDAS,Risk,Tick-by-tick applications,Volatility},
language = {en},
mendeley-tags = {MIDAS},
month = feb,
number = {1},
pages = {53--90},
publisher = {Taylor \& Francis Group},
title = {{MIDAS Regressions: Further Results and New Directions}},
url = {http://www.tandfonline.com/doi/abs/10.1080/07474930600972467},
volume = {26},
year = {2007}
}
@article{Ghysels2009,
abstract = {Surveys of forecasters, containing respondents’ predictions of future values of key macroeconomic variables, receive a lot of attention in the financial press, from investors and from policy makers. They are apparently widely perceived to provide useful information about agents’ expectations. Nonetheless, these survey forecasts suffer from the crucial disadvantage that they are often quite stale, as they are released only infrequently. In this article, we propose MIDAS regression and Kalman filter methods for using asset price data to construct daily forecasts of upcoming survey releases. Our methods also allow us to predict actual outcomes, providing competing forecasts, and allow us to estimate what professional forecasters would predict if they were asked to make a forecast each day, making it possible to measure the effects of events and news announcements on expectations.},
author = {Ghysels, Eric and Wright, Jonathan H.},
doi = {10.1198/jbes.2009.06044},
file = {:C$\backslash$:/Users/snowdj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ghysels, Wright - 2009 - Forecasting Professional Forecasters.pdf:pdf},
issn = {0735-0015},
journal = {Journal of Business \& Economic Statistics},
keywords = {Forecast evaluation,Kalman filter,Mixed frequency data sampling,News announcements,Survey forecasts},
language = {en},
month = oct,
number = {4},
pages = {504--516},
publisher = {Taylor \& Francis},
title = {{Forecasting Professional Forecasters}},
url = {http://amstat.tandfonline.com/doi/abs/10.1198/jbes.2009.06044},
volume = {27},
year = {2009}
}
@article{Giacomini,
author = {Giacomini, Raffaella},
title = {{Economic theory and forecasting : lessons from the literature Economic theory and forecasting : lessons from the literature}}
}
@article{Giacomini2010,
author = {Giacomini, Raffaella and Rossi, Barbara},
doi = {10.1002/jae.1177},
file = {:C$\backslash$:/Users/snowdj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Giacomini, Rossi - 2010 - Forecast comparisons in unstable environments.pdf:pdf},
issn = {08837252},
journal = {Journal of Applied Econometrics},
month = jun,
number = {4},
pages = {595--620},
title = {{Forecast comparisons in unstable environments}},
url = {http://doi.wiley.com/10.1002/jae.1177},
volume = {25},
year = {2010}
}
@article{Giannone2008,
abstract = {A formal method is developed for evaluating the marginal impact that intra-monthly data releases have on current-quarter forecasts (nowcasts) of real gross domestic product (GDP) growth. The method can track the real-time flow of the type of information monitored by central banks because it can handle large data sets with staggered data-release dates. Each time new data are released, the nowcasts are updated on the basis of progressively larger data sets that, reflecting the unsynchronized data-release dates, have a “jagged edge” across the most recent months.},
author = {Giannone, Domenico and Reichlin, Lucrezia and Small, David},
doi = {10.1016/j.jmoneco.2008.05.010},
issn = {03043932},
journal = {Journal of Monetary Economics},
keywords = {C33,C53,E52,Factor model,Forecasting,Monetary policy,Nowcast,Real-time data},
month = may,
number = {4},
pages = {665--676},
title = {{Nowcasting: The real-time informational content of macroeconomic data}},
url = {http://www.sciencedirect.com/science/article/pii/S0304393208000652},
volume = {55},
year = {2008}
}
@book{Greene2003,
abstract = {Econometric Analysisi, 6/eserves as a bridge between an introduction to the field of econometrics and the professional literature for social scientists and other professionals in the field of social sciences, focusing on applied econometrics and theoretical background. This book provides a broad survey of the field of econometrics that allows the reader to move from here to practice in one or more specialized areas. At the same time, the reader will gain an appreciation of the common foundation of all the fields presented and use the tools they employ.This book gives space to a wide range of topics including basic econometrics, Classical, Bayesian, GMM, and Maximum likelihood, and gives special emphasis to new topics such a time series and panels.For social scientists and other professionals in the field who want a thorough introduction to applied econometrics that will prepare them for advanced study and practice in the field.},
author = {Greene, William H},
booktitle = {Journal of the American Statistical Association},
doi = {10.1198/jasa.2002.s458},
editor = {Education, Pearson},
isbn = {0130661899},
issn = {01621459},
number = {457},
pages = {1026},
pmid = {21414993},
publisher = {Prentice Hall},
title = {{Econometric Analysis}},
url = {http://pubs.amstat.org/doi/abs/10.1198/jasa.2002.s458},
volume = {97},
year = {2003}
}
@article{Guerin2013,
abstract = {This article introduces a new regression model—Markov-switching mixed data sampling (MS-MIDAS)—that incorporates regime changes in the parameters of the mixed data sampling (MIDAS) models and allows for the use of mixed-frequency data in Markov-switching models. After a discussion of estimation and inference for MS-MIDAS and a small sample simulation-based evaluation, the MS-MIDAS model is applied to the prediction of the U.S. economic activity, in terms of both quantitative forecasts of the aggregate economic activity and the prediction of the business cycle regimes. Both simulation and empirical results indicate that MS-MIDAS is a very useful specification.},
author = {Gu\'{e}rin, Pierre and Marcellino, Massimiliano},
isbn = {10.1080/07350015.2012.727721},
keywords = {Business cycle,Forecasting,Mixed-frequency data,Nonlinear models,Nowcasting},
month = jan,
publisher = {Taylor \& Francis Group},
title = {{Markov-Switching MIDAS Models}},
url = {http://www.tandfonline.com/doi/abs/10.1080/07350015.2012.727721},
year = {2013}
}
@article{Hansen2013,
author = {Hansen, Bruce E},
keywords = {2012,and participants at the,and the 2013 nber,cireq econometrics conference,co-editor,cross-validation,factor models,for helpful comments,forecast combination,frank diebold,frank schorfheide,generated regressors,hansen thanks the national,mallows,science foundation for research,summer institute forecasting seminar,support,the 2012 cesg meeting,the authors thank the,three referees},
pages = {1--37},
title = {{Forecasting with Factor-Augmented Regression : A Frequentist Model Averaging Approach}},
year = {2013}
}
@misc{Harvey2006a,
abstract = {Structural time series models are formulated in terms of components, such as trends, seasonals and cycles, that have a direct interpretation. As well as providing a framework for time series decomposition by signal extraction, they can be used for forecasting and for 'nowcasting'. The structural interpretation allows extensions to classes of models that are able to deal with various issues in multivariate series and to cope with non-Gaussian observations and nonlinear models. The statistical treatment is by the state space form and hence data irregularities such as missing observations are easily handled. Continuous time models offer further flexibility in that they can handle irregular spacing. The paper compares the forecasting performance of structural time series models with ARIMA and autoregressive models. Results are presented showing how observations in linear state space models are implicitly weighted in making forecasts and hence how autoregressive and vector error correction representations can be obtained. The use of an auxiliary series in forecasting and nowcasting is discussed. A final section compares stochastic volatility models with GARCH. ?? 2006 Elsevier B.V. All rights reserved.},
author = {Harvey, Andrew},
booktitle = {Handbook of Economic Forecasting},
doi = {10.1016/S1574-0706(05)01007-4},
file = {:C$\backslash$:/Users/snowdj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Harvey - 2006 - Chapter 7 Forecasting with Unobserved Components Time Series Models.pdf:pdf},
isbn = {9780444513953},
issn = {15740706},
keywords = {Kalman filter,continuous time,cycles,non-Gaussian models,state space,stochastic trend,stochastic volatility},
pages = {327--412},
pmid = {11465069},
title = {{Chapter 7 Forecasting with Unobserved Components Time Series Models}},
volume = {1},
year = {2006}
}
@book{Harvey1990a,
abstract = {In this book, Andrew Harvey sets out to provide a unified and comprehensive theory of structural time series models. Unlike the traditional ARIMA models, structural time series models consist explicitly of unobserved components, such as trends and seasonals, which have a direct interpretation. As a result the model selection methodology associated with structural models is much closer to econometric methodology. The link with econometrics is made even closer by the natural way in which the models can be extended to include explanatory variables and to cope with multivariate time series. From the technical point of view, state space models and the Kalman filter play a key role in the statistical treatment of structural time series models. The book includes a detailed treatment of the Kalman filter. This technique was originally developed in control engineering, but is becoming increasingly important in fields such as economics and operations research. This book is concerned primarily with modelling economic and social time series, and with addressing the special problems which the treatment of such series poses. The properties of the models and the methodological techniques used to select them are illustrated with various applications. These range from the modellling of trends and cycles in US macroeconomic time series to to an evaluation of the effects of seat belt legislation in the UK.},
author = {Harvey, Andrew},
isbn = {1107717140},
publisher = {Cambridge University Press},
title = {{Forecasting, Structural Time Series Models and the Kalman Filter}},
url = {https://books.google.com/books?hl=zh-CN\&lr=\&id=zkGdAgAAQBAJ\&pgis=1},
year = {1990}
}
@misc{Harvey2006a,
abstract = {Structural time series models are formulated in terms of components, such as trends, seasonals and cycles, that have a direct interpretation. As well as providing a framework for time series decomposition by signal extraction, they can be used for forecasting and for 'nowcasting'. The structural interpretation allows extensions to classes of models that are able to deal with various issues in multivariate series and to cope with non-Gaussian observations and nonlinear models. The statistical treatment is by the state space form and hence data irregularities such as missing observations are easily handled. Continuous time models offer further flexibility in that they can handle irregular spacing. The paper compares the forecasting performance of structural time series models with ARIMA and autoregressive models. Results are presented showing how observations in linear state space models are implicitly weighted in making forecasts and hence how autoregressive and vector error correction representations can be obtained. The use of an auxiliary series in forecasting and nowcasting is discussed. A final section compares stochastic volatility models with GARCH. ?? 2006 Elsevier B.V. All rights reserved.},
author = {Harvey, Andrew},
booktitle = {Handbook of Economic Forecasting},
doi = {10.1016/S1574-0706(05)01007-4},
isbn = {9780444513953},
issn = {15740706},
keywords = {Kalman filter,continuous time,cycles,non-Gaussian models,state space,stochastic trend,stochastic volatility},
pages = {327--412},
pmid = {11465069},
title = {{Chapter 7 Forecasting with Unobserved Components Time Series Models}},
volume = {1},
year = {2006}
}
@article{Harvey,
abstract = {By setting up a suitable time series model in state space form, the latest estimate of the underlying current change in a series may be computed by the Kalman ®lter. This may be done even if the observations are only available in a time-aggregated form subject to survey sampling error. A related series, possibly observed more frequently, may be used to improve the estimate of change further. The paper applies these techniques to the important problem of estimating the underlying monthly change in unemployment in the UK measured according to the de®nition of the International Labour Organisation by the Labour Force Survey. The ®tted models suggest a reduction in root-mean-squared error of around 10\% over a simple estimate based on differences if a univariate model is used and a further reduction of 50\% if information on claimant counts is taken into account. With seasonally unadjusted data, the bivariate model offers a gain of roughly 40\% over the use of annual differences. For both adjusted and unadjusted data, there is a further gain of around 10\% if the next month's ®gure on claimant counts is used. The method preferred is based on a bivariate model with unadjusted data. If the next month's claimant count is known, the root-mean-squared error for the estimate of change is just over 10 000.},
author = {Harvey, Andrew and Chung, Chia-Hui},
file = {:C$\backslash$:/Users/snowdj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Harvey, Chung - Unknown - Estimating the underlying change in unemployment in the UK.pdf:pdf},
keywords = {Co-integration,Common trends,Kalman ®lter,Mixed frequency data,Rotating sample,Survey data},
title = {{Estimating the underlying change in unemployment in the UK}}
}
@article{Harvey1990,
author = {Harvey, Andrew and Peters, S},
file = {:E$\backslash$:/Dropbox/phd/statespace/Harvey\_Peters1990.pdf:pdf},
journal = {Journal of Forecasting},
number = {June 1988},
pages = {89--108},
title = {{Estimation Procedures for Structural Time Series Models}},
volume = {9},
year = {1990}
}
@article{Hassani2015,
author = {Hassani, Hossein and Silva, Emmanuel Sirimal},
doi = {10.1007/s40745-015-0029-9},
file = {:C$\backslash$:/Users/snowdj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hassani, Silva - 2015 - Forecasting with Big Data A Review.pdf:pdf},
issn = {2198-5804},
journal = {Annals of Data Science},
month = apr,
title = {{Forecasting with Big Data: A Review}},
url = {http://link.springer.com/10.1007/s40745-015-0029-9},
year = {2015}
}
@article{Heckman1979,
abstract = {discusses the bias that results from using nonrandomly selected samples to estimate behavioral relationships as an ordinary specification error or “omitted variable” bias; simple consistent two-stage estimator is considered},
author = {Heckman, James J.},
doi = {10.2307/1912352},
isbn = {00129682},
issn = {00129682},
journal = {Econometrica},
number = {1},
pages = {153--161},
pmid = {1912352},
title = {{Sample Selection Bias as a Specification Error}},
volume = {47},
year = {1979}
}
@article{Hendry2014,
abstract = {Big Data offer potential benefits for statistical modelling, but confront problems like an excess of false positives, mistaking correlations for causes, ignoring sampling biases, and selecting by inappropriate methods.� We consider the many important requirements when searching for a data-based relationship using Big Data, and the possible role of Autometrics in that context.� Paramount considerations include embedding relationships in general initial models, possibly restricting the number of variables to be selected over by non-statistical criteria (the formulation problem), using good quality data on all variables, analyzed with tight significance levels by a powerful selection procedure, retaining available theory insights (the selection problem) while testing for relationships being well specified and invariant to shifts in explanatory variables (the evaluation problem), using a viable approach that resolves the computational problem of immense numbers of possible models.},
author = {Hendry, David and Doornik, Jurgen A.},
file = {:C$\backslash$:/Users/snowdj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hendry, Doornik - 2014 - Statistical Model Selection with 'Big Data'.pdf:pdf},
journal = {Economics Series Working Papers},
keywords = {Autometrics,Big Data,Location Shifts,Model Selection},
month = dec,
publisher = {University of Oxford, Department of Economics},
title = {{Statistical Model Selection with 'Big Data'}},
url = {http://ideas.repec.org/p/oxf/wpaper/735.html},
year = {2014}
}
@misc{Hendry2011,
abstract = {To forecast an aggregate, we propose adding disaggregate variables, instead of combining forecasts of those disaggregates or forecasting by a univariate aggregate model. New analytical results show the effects of changing coefficients, mis-specification, estimation uncertainty and mis-measurement error. Forecastorigin shifts in parameters affect absolute, but not relative, forecast accuracies; mis-specification and estimation uncertainty induce forecast-error differences, which variable-selection procedures or dimension reductions can mitigate. In Monte Carlo simulations, different stochastic structures and interdependencies between disaggregates imply that including disaggregate information in the aggregate model improves forecast accuracy. Our theoretical predictions and simulations are corroborated when forecasting aggregate US inflation pre- and post 1984 using disaggregate sectoral data. JEL Classification: C51, C53, E31. (This abstract was borrowed from another version of this item.)},
author = {Hendry, David and Hubrich, Kirstin},
booktitle = {Journal of Business \& Economic Statistics},
doi = {10.1198/jbes.2009.07112},
issn = {0735-0015},
title = {{Combining Disaggregate Forecasts or Combining Disaggregate Information to Forecast an Aggregate}},
year = {2011}
}
@article{Hendry2012,
abstract = {Understanding the workings of whole economies is essential for sound policy advice - but not necessarily for accurate forecasts.� Structural models play a major role at most central banks and many other governmental agencies, yet almost none forecast the financial crisis and ensuing recession.� We focus on the problem of forecast failure that has become prominent during and after that crisis, and illustrate its sources and many surprising implications using a simple model.� An application to 'forecasting' UK GDP over 2008(1)-2011(2) is consistent with our interpretation.},
author = {Hendry, David and Mizon, Grayham E.},
file = {:C$\backslash$:/Users/snowdj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hendry, Mizon - 2012 - Forecasting from Structural Econometric Models.pdf:pdf},
journal = {Economics Series Working Papers},
keywords = {Autometrics,Economic forecasting,Location shifts,Structural models},
month = mar,
publisher = {University of Oxford, Department of Economics},
title = {{Forecasting from Structural Econometric Models}},
url = {http://ideas.repec.org/p/oxf/wpaper/597.html},
year = {2012}
}
@misc{Hyndman2015,
annote = {R package version 6.1},
author = {Hyndman, Rob J},
title = {{\{forecast\}: Forecasting functions for time series and linear models}},
url = {http://github.com/robjhyndman/forecast},
year = {2015}
}
@article{Hyndman2002a,
abstract = {We provide a new approach to automatic forecasting based on an extended range of exponential smoothing methods. Each method in our taxonomy of exponential smoothing methods provides forecasts that are equivalent to forecasts from a state space model. This equivalence allows: (1) easy calculation of the likelihood, the AIC and other model selection criteria; (2) computation of prediction intervals for each method; and (3) random simulation from the underlying state space model. We demonstrate the methods by applying them to the data from the M-competition and the M3-competition. The method provides forecast accuracy comparable to the best methods in the competitions; it is particularly good for short forecast horizons with seasonal data. ?? 2002 International Institute of Forecasters. Published by Elsevier Science B.V. All rights reserved.},
author = {Hyndman, Rob J. and Koehler, Anne B. and Snyder, Ralph D. and Grose, Simone},
doi = {10.1016/S0169-2070(01)00110-8},
file = {:C$\backslash$:/Users/snowdj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hyndman et al. - 2002 - A state space framework for automatic forecasting using exponential smoothing methods.pdf:pdf},
isbn = {0169-2070},
issn = {01692070},
journal = {International Journal of Forecasting},
keywords = {Automatic forecasting,Exponential smoothing,Prediction intervals,State space models},
title = {{A state space framework for automatic forecasting using exponential smoothing methods}},
year = {2002}
}
@article{Hyndman2008,
author = {Hyndman, Rob J and Khandakar, Yeasmin},
journal = {Journal of Statistical Software},
number = {3},
pages = {1--22},
title = {{Automatic time series forecasting: the forecast package for \{R\}}},
url = {http://ideas.repec.org/a/jss/jstsof/27i03.html},
volume = {26},
year = {2008}
}
@book{Inoue2005,
address = {London},
author = {Inoue, Atsushi},
publisher = {Centre for Economic Policy Research},
title = {{How useful is bagging in forecasting economic time series? : a case study of US CPI inflation}},
year = {2005}
}
@article{Inoue2008,
abstract = {This article focuses on the widely studied question of whether the inclusion of indicators of real economic activity lowers the prediction mean squared error of forecasting models of U.S. consumer price inflation. We propose three variants of the bagging algorithm specifically designed for this type of forecasting problem and evaluate their empirical performance. Although bagging predictors in our application are clearly more accurate than equally weighted forecasts, median forecasts, ARM forecasts, AFTER forecasts, or Bayesian forecast averages based on one extra predictor at a time, they are generally about as accurate as the Bayesian shrinkage predictor, the ridge regression predictor, the iterated LASSO predictor, or the Bayesian model average predictor based on random subsets of extra predictors. Our results show that bagging can achieve large reductions in prediction mean-squared errors even in such challenging applications as inflation forecasting; however, bagging is not the only method capable of...},
author = {Inoue, Atsushi and Kilian, Lutz},
doi = {10.1198/016214507000000473},
file = {:C$\backslash$:/Users/snowdj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Inoue, Kilian - 2008 - How Useful Is Bagging in Forecasting Economic Time Series A Case Study of U.S. Consumer Price Inflation.pdf:pdf},
issn = {0162-1459},
journal = {Journal of the American Statistical Association},
keywords = {Bayesian model averaging,Bootstrap aggregation,Factor model,Forecast combination,Forecasting model selection,Pre testing,Shrinkage estimation},
language = {en},
month = jun,
number = {482},
pages = {511--522},
publisher = {Taylor \& Francis},
title = {{How Useful Is Bagging in Forecasting Economic Time Series? A Case Study of U.S. Consumer Price Inflation}},
url = {http://www.tandfonline.com/doi/abs/10.1198/016214507000000473},
volume = {103},
year = {2008}
}
@article{Iteration,
author = {Iteration, Iteration},
title = {{The above results show that using both the mfx and prchange commands , we can get the same result of the marginal effect of age on the Pr ( ins = 1 | x ).}}
}
@article{Jalles2009,
author = {Jalles, Jt},
doi = {10.2139/ssrn.1496864},
file = {:E$\backslash$:/Dropbox/phd/statespace/Structural Time Series Models and the Kalman Filter.pdf:pdf},
issn = {1556-5068},
journal = {FEUNL Work Pap},
keywords = {cointegration,likelihood,smoothing,sutse},
pages = {1--30},
title = {{Structural time series models and the Kalman Filter: a concise review}},
url = {http://fesrvsd.fe.unl.pt/WPFEUNL/WP2009/wp541.pdf},
year = {2009}
}
@article{JennieBai,
author = {{Jennie Bai Eric Ghysels}, Jonathan H Wright},
title = {{State Space Models and MIDAS Regressions ∗}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.184.4807}
}
@article{Bai2013,
author = {{Jennie Bai, Eric Ghysels}, Jonathan H. Wright and Bai, Jennie and Ghysels, Eric and Wright, Jonathan H. and {Jennie Bai Eric Ghysels}, Jonathan H Wright},
doi = {10.1080/07474938.2012.690675},
file = {:E$\backslash$:/Dropbox/phd/statespace/State Space Models and MIDAS Regressions.pdf:pdf;:C$\backslash$:/Users/snowdj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Jennie Bai - Unknown - State Space Models and MIDAS Regressions ∗.pdf:pdf},
issn = {0747-4938},
journal = {Econometric Reviews},
number = {7},
pages = {779--813},
title = {{State Space Models and MIDAS Regressions ∗}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.184.4807 http://www.tandfonline.com/doi/abs/10.1080/07474938.2012.690675},
volume = {32},
year = {2013}
}
@article{Jungbacker2011,
abstract = {This paper concerns estimating parameters in a high-dimensional dynamic factor model by the method of maximum likelihood. To accommodate missing data in the analysis, we propose a new model representation for the dynamic factor model. It allows the Kalman filter and related smoothing methods to evaluate the likelihood function and to produce optimal factor estimates in a computationally efficient way when missing data is present. The implementation details of our methods for signal extraction and maximum likelihood estimation are discussed. The computational gains of the new devices are presented based on simulated data sets with varying numbers of missing entries. ?? 2011 Elsevier B.V.},
author = {Jungbacker, B. and Koopman, S.J. J. and van der Wel, M.},
doi = {10.1016/j.jedc.2011.03.009},
issn = {01651889},
journal = {Journal of Economic Dynamics and Control},
keywords = {High-dimensional vector series,Kalman filtering and smoothing,Unbalanced panels of time series},
number = {8},
pages = {1358--1368},
title = {{Maximum likelihood estimation for dynamic factor models with missing data}},
volume = {35},
year = {2011}
}
@article{Kahneman1979,
abstract = {This paper presents a critique of expected utility theory as a descriptive model of decision making under risk, and develops an alternative model, called prospect theory. Choices among risky prospects exhibit several pervasive effects that are inconsistent with the basic tenets of utility theory. In particular, people underweight outcomes that are merely probable in comparison with outcomes that are obtained with certainty. This tendency, called the certainty effect, contributes to risk aversion in choices involving sure gains and to risk seeking in choices involving sure losses. In addition, people generally discard components that are shared by all prospects under consideration. This tendency, called the isolation effect, leads to inconsistent preferences when the same choice is presented in different forms. An alternative theory of choice is developed, in which value is assigned to gains and losses rather than to final assets and in which probabilities are replaced by decision weights. The value function is normally concave for gains, commonly convex for losses, and is generally steeper for losses than for gains. Decision weights are generally lower than the corresponding probabilities, except in the range of low prob- abilities. Overweighting of low probabilities may contribute to the attractiveness of both insurance and gambling. 1.},
author = {Kahneman, Daniel and Tversky, Amos},
doi = {10.2307/1914185},
isbn = {0521627494},
issn = {00129682},
journal = {Econometrica},
number = {2},
pages = {263--292},
pmid = {1914185},
title = {{Prospect Theory: An Analysis of Decision under Risk}},
url = {http://medcontent.metapress.com/index/A65RM03P4874243N.pdf},
volume = {47},
year = {1979}
}
@article{Kaufmann2012,
abstract = {This paper considers factor estimation from heterogenous data, where some of the variables are noisy and only weakly informative for the factors. To identify the irrelevant variables, we search for zero rows in the loadings matrix of the factor model. To sharply separate these irrelevant variables from the informative ones, we choose a Bayesian framework for factor estimation with sparse priors on the loadings matrix. The choice of a sparse prior is an extension to the existing macroeconomic literature, which predominantly uses normal priors on the loadings. Simulations show that the sparse factor model can well detect various degrees of sparsity in the data, and how irrelevant variables can be identified. Empirical applications to a large multi-country GDP dataset and disaggregated CPI inflation data for the US reveal that sparsity matters a lot, as the majority of the variables in both datasets are irrelevant for factor estimation.},
author = {Kaufmann, Sylvia and Schumacher, Christian},
file = {:C$\backslash$:/Users/snowdj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kaufmann, Schumacher - 2012 - Finding relevant variables in sparse Bayesian factor models Economic applications and simulation results.pdf:pdf},
journal = {Technical Report 29, Deutsche Bundesbank Discussion Paper.},
keywords = {factor models,sparse priors,variable selection},
publisher = {Deutsche Bundesbank, Research Centre},
title = {{Finding relevant variables in sparse Bayesian factor models: Economic applications and simulation results}},
url = {http://ideas.repec.org/p/zbw/bubdps/292012.html},
year = {2012}
}
@book{Koop2010b,
abstract = {Bayesian Multivariate Time Series Methods for Empirical Macroeconomics provides a survey of the Bayesian methods used in modern empirical macroeconomics. These models have been developed to address the fact that most questions of interest to empirical macroeconomists involve several variables and must be addressed using multivariate time series methods. Many different multivariate time series models have been used in macroeconomics, but Vector Autoregressive (VAR) models have been among the most popular. Bayesian Multivariate Time Series Methods for Empirical Macroeconomics reviews and extends the Bayesian literature on VARs, TVP-VARs and TVP-FAVARs with a focus on the practitioner. The authors go beyond simply defining each model, but specify how to use them in practice, discuss the advantages and disadvantages of each and offer tips on when and why each model can be used.},
author = {Koop, Gary and Korobilis, Dimitris},
file = {:C$\backslash$:/Users/snowdj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Koop, Korobilis - 2010 - Bayesian Multivariate Time Series Methods for Empirical Macroeconomics.pdf:pdf},
isbn = {160198362X},
pages = {94},
publisher = {Now Publishers Inc},
title = {{Bayesian Multivariate Time Series Methods for Empirical Macroeconomics}},
url = {https://books.google.com/books?hl=en\&lr=\&id=JcVn9IJ9wSsC\&pgis=1},
year = {2010}
}
@article{Koop2012,
author = {Koop, Gary and Korobilis, Dimitris},
doi = {10.1111/j.1468-2354.2012.00704.x},
file = {:C$\backslash$:/Users/snowdj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Koop, Korobilis - 2012 - FORECASTING INFLATION USING DYNAMIC MODEL AVERAGING.pdf:pdf},
issn = {00206598},
journal = {International Economic Review},
month = aug,
number = {3},
pages = {867--886},
title = {{FORECASTING INFLATION USING DYNAMIC MODEL AVERAGING*}},
url = {http://doi.wiley.com/10.1111/j.1468-2354.2012.00704.x},
volume = {53},
year = {2012}
}
@article{Koop2010a,
author = {Koop, Gary and Korobilis, Dimitris},
file = {:C$\backslash$:/Users/snowdj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Koop, Korobilis - 2010 - Forecasting In inflation using dynamic model averaging.pdf:pdf},
journal = {International Economic Review},
keywords = {20 th ec 2,analysis,bayesian,c11,c53,cation,conference in real-time econometrics,e31,e37,eurostat colloquim on modern,jel classi,phillips curve,state space model,thank participants at the,the,the 6 th,the european central bank,tools for business cycle,we would like to},
number = {3},
pages = {867--886},
title = {{Forecasting In inflation using dynamic model averaging}},
volume = {53},
year = {2010}
}
@article{Koop2004,
author = {Koop, Gary and Potter, Simon},
doi = {10.1111/j.1368-423X.2004.00143.x},
file = {:C$\backslash$:/Users/snowdj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Koop, Potter - 2004 - Forecasting in dynamic factor models using Bayesian model averaging.pdf:pdf},
issn = {1368-4221},
journal = {The Econometrics Journal},
keywords = {bayesian model averaging,composition,diffusion index,markov chain monte carlo,model,reference prior},
month = dec,
number = {2},
pages = {550--565},
title = {{Forecasting in dynamic factor models using Bayesian model averaging}},
url = {http://doi.wiley.com/10.1111/j.1368-423X.2004.00143.x http://onlinelibrary.wiley.com/doi/10.1111/j.1368-423X.2004.00143.x/full},
volume = {7},
year = {2004}
}
@article{Koopman2012,
author = {Koopman, Siem Jan and Ooms, Marius},
doi = {10.1093/oxfordhb/9780195398649.013.0006},
file = {:E$\backslash$:/Dropbox/phd/statespace/SiemJanKoopman-final-2010UCForecasting.pdf:pdf},
isbn = {9780199940325},
title = {{Forecasting Economic Time Series Using Unobserved Components Time Series Models}},
url = {http://oxfordhandbooks.com/view/10.1093/oxfordhb/9780195398649.001.0001/oxfordhb-9780195398649-e-6},
year = {2012}
}
@article{Koopman2013,
abstract = {We extend the class of dynamic factor yield curve models in order to include macroeconomic factors. Our work benefits from recent developments in the dynamic factor literature related to the extraction of the common factors from a large panel of macroeconomic series and the estimation of the parameters in the model. We include these factors in a dynamic factor model for the yield curve, in which we model the salient structure of the yield curve by imposing smoothness restrictions on the yield factor loadings via cubic spline functions. We carry out a likelihood-based analysis in which we jointly consider a factor model for the yield curve, a factor model for the macroeconomic series, and their dynamic interactions with the latent dynamic factors. We illustrate the methodology by forecasting the U.S. term structure of interest rates. For this empirical study, we use a monthly time series panel of unsmoothed Fama–Bliss zero yields for treasuries of different maturities between 1970 and 2009, which we combine with a macro panel of 110 series over the same sample period. We show that the relationship between the macroeconomic factors and the yield curve data has an intuitive interpretation, and that there is interdependence between the yield and macroeconomic factors. Finally, we perform an extensive out-of-sample forecasting study. Our main conclusion is that macroeconomic variables can lead to more accurate yield curve forecasts.},
author = {Koopman, Siem Jan and van der Wel, Michel},
doi = {10.1016/j.ijforecast.2012.12.004},
issn = {01692070},
journal = {International Journal of Forecasting},
keywords = {Fama–Bliss data set,Kalman filter,Maximum likelihood,Yield curve},
month = oct,
number = {4},
pages = {676--694},
title = {{Forecasting the US term structure of interest rates using a macroeconomic smooth dynamic factor model}},
url = {http://www.sciencedirect.com/science/article/pii/S0169207013000058},
volume = {29},
year = {2013}
}
@article{Korobilis2013,
abstract = {This paper examines the properties of Bayes shrinkage estimators for dynamic regressions that are based on hierarchical versions of the typical normal prior. Various popular penalized least squares estimators for shrinkage and selection in regression models can be recovered using a single hierarchical Bayes formulation. Using 129 US macroeconomic quarterly variables for the period 1959–2010, I extensively evaluate the forecasting properties of Bayesian shrinkage in macroeconomic forecasting with many predictors. The results show that, for particular data series, hierarchical shrinkage dominates factor model forecasts, and hence serves as a valuable addition to the existing methods for handling large dimensional data.},
author = {Korobilis, Dimitris},
doi = {10.1016/j.ijforecast.2012.05.006},
file = {:C$\backslash$:/Users/snowdj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Korobilis - 2013 - Hierarchical shrinkage priors for dynamic regressions with many predictors.pdf:pdf},
issn = {01692070},
journal = {International Journal of Forecasting},
keywords = {Bayesian lasso,Factor model,Forecasting,Shrinkage,Variable selection},
month = jan,
number = {1},
pages = {43--59},
title = {{Hierarchical shrinkage priors for dynamic regressions with many predictors}},
url = {http://www.sciencedirect.com/science/article/pii/S0169207012000817},
volume = {29},
year = {2013}
}
@misc{Krugman1991,
abstract = {This paper develops a simple model that shows how a country can endogenously become differentiated into an industrialized "core" and an agricultural "periphery." In order to realize scale economies while minimizing transport costs, manufacturing firms tend to locate in the region with larger demand, but the location of demand itself depends on the distribution of manufacturing. Emergence of a core-periphery pattern depends on transportations costs, economies of scale, and the share of manufacturing in national income.},
author = {Krugman, Paul},
booktitle = {Journal of Political Economy},
doi = {10.1086/261763},
isbn = {00223808},
issn = {0022-3808},
number = {3},
pages = {483},
pmid = {1416},
title = {{Increasing Returns and Economic Geography}},
volume = {99},
year = {1991}
}
@article{Kuzin2011a,
abstract = {This paper compares the mixed-data sampling (MIDAS) and mixed-frequency VAR (MF-VAR) approaches to model specification in the presence of mixed-frequency data, e.g. monthly and quarterly series. MIDAS leads to parsimonious models which are based on exponential lag polynomials for the coefficients, whereas MF-VAR does not restrict the dynamics and can therefore suffer from the curse of dimensionality. However, if the restrictions imposed by MIDAS are too stringent, the MF-VAR can perform better. Hence, it is difficult to rank MIDAS and MF-VAR a priori, and their relative rankings are better evaluated empirically. In this paper, we compare their performances in a case which is relevant for policy making, namely nowcasting and forecasting quarterly GDP growth in the euro area on a monthly basis, using a set of about 20 monthly indicators. It turns out that the two approaches are more complements than substitutes, since MIDAS tends to perform better for horizons up to four to five months, whereas MF-VAR performs better for longer horizons, up to nine months. ?? 2010 International Institute of Forecasters.},
author = {Kuzin, Vladimir and Marcellino, Massimiliano and Schumacher, Christian},
doi = {10.1016/j.ijforecast.2010.02.006},
file = {:E$\backslash$:/Dropbox/phd/MIDAS/MIDAS vs. mixed-frequency VAR.pdf:pdf},
issn = {01692070},
journal = {International Journal of Forecasting},
keywords = {MIDAS,Mixed-frequency VAR,Mixed-frequency data,Nowcasting},
number = {2},
pages = {529--542},
publisher = {Elsevier B.V.},
title = {{MIDAS vs. mixed-frequency VAR: Nowcasting GDP in the euro area}},
url = {http://dx.doi.org/10.1016/j.ijforecast.2010.02.006},
volume = {27},
year = {2011}
}
@article{Kvedaras2012,
abstract = {We propose a test for the evaluation of statistical acceptability of a functional constraint which is imposed on parameters in the mixed data sampling regressions. The asymptotic behavior of the test statistic is characterized and a few other extensions are discussed. ?? 2012 Elsevier B.V.},
author = {Kvedaras, Virmantas and Zemlys, Vaidotas},
doi = {10.1016/j.econlet.2012.03.009},
issn = {01651765},
journal = {Economics Letters},
keywords = {Constraint,MIDAS,Restriction,Specification,Test},
number = {2},
pages = {250--254},
publisher = {Elsevier B.V.},
title = {{Testing the functional constraints on parameters in regressions with variables of different frequency}},
url = {http://dx.doi.org/10.1016/j.econlet.2012.03.009},
volume = {116},
year = {2012}
}
@article{Lampos2012,
abstract = {We present a general methodology for inferring the occurrence and magnitude of an event or phenomenon by exploring the rich amount of unstructured textual information on the social part of the web. Having geo-tagged user posts on the microblogging service of Twitter as our input data, we investigate two case studies. The first consists of a benchmark problem, where actual levels of rainfall in a given location and time are inferred from the content of tweets. The second one is a real-life task, where we infer regional Influenza-like Illness rates in the effort of detecting timely an emerging epidemic disease. Our analysis builds on a statistical learning framework, which performs sparse learning via the bootstrapped version of LASSO to select a consistent subset of textual features from a large amount of candidates. In both case studies, selected features indicate close semantic correlation with the target topics and inference, conducted by regression, has a significant performance, especially given the short length — approximately one year — of Twitter's data time series.},
author = {Lampos, Vasileios and Cristianini, Nello},
doi = {10.1145/2337542.2337557},
file = {:E$\backslash$:/Dropbox/phd/forecast/Nowcasting with statistical learning.pdf:pdf},
issn = {21576904},
journal = {ACM Transactions on Intelligent Systems and Technology},
number = {4},
pages = {1--22},
title = {{Nowcasting Events from the Social Web with Statistical Learning}},
volume = {3},
year = {2012}
}
@article{Lee2013,
abstract = {Bayesian inference has become a standard method of analysis in many fields of science. Students and researchers in experimental psychology and cognitive science, however, have failed to take full advantage of the new and exciting possibilities that the Bayesian approach affords. Ideal for teaching and self study, this book demonstrates how to do Bayesian modeling. Short, to-the-point chapters offer examples, exercises and computer code (using WinBUGS or JAGS, and supported by Matlab and R), with additional support available online. No advance knowledge of statistics is required and, from the very start, readers are encouraged to apply and adjust Bayesian analyses by themselves. The book contains a series of chapters on parameter estimation and model selection, followed by detailed case studies from cognitive science. After working through this book, readers should be able to build their own Bayesian models, apply the models to their own data and draw their own conclusions.},
author = {Lee, Michael D and Wagenmakers, Eric-Jan},
isbn = {978-1-107-01845-7},
number = {2010},
pages = {265},
title = {{Bayesian Cognitive Modeling : A Practical Course}},
year = {2013}
}
@misc{Madigan1994a,
abstract = {Abstract We consider the problem of model selection and accounting for model uncertainty in high-dimensional contingency tables, motivated by expert system applications. The approach most used currently is a stepwise strategy guided by tests based on approximate asymptotic P values leading to the selection of a single model; inference is then conditional on the selected model. The sampling properties of such a strategy are complex, and the failure to take account of model uncertainty leads to underestimation of uncertainty about quantities of interest. In principle, a panacea is provided by the standard Bayesian formalism that averages the posterior distributions of the quantity of interest under each of the models, weighted by their posterior model probabilities. Furthermore, this approach is optimal in the sense of maximizing predictive ability. But this has not been used in practice, because computing the posterior model probabilities is hard and the number of models is very large (often greater than 1011). We argue that the standard Bayesian formalism is unsatisfactory and propose an alternative Bayesian approach that, we contend, takes full account of the true model uncertainty by averaging over a much smaller set of models. An efficient search algorithm is developed for finding these models. We consider two classes of graphical models that arise in expert systems: the recursive causal models and the decomposable log-linear models. For each of these, we develop efficient ways of computing exact Bayes factors and hence posterior model probabilities. For the decomposable log-linear models, this is based on properties of chordal graphs and hyper-Markov prior distributions and the resultant calculations can be carried out locally. The end product is an overall strategy for model selection and accounting for model uncertainty that searches efficiently through the very large classes of models involved. Three examples are given. The first two concern data sets that have been analyzed by several authors in the context of model selection. The third addresses a urological diagnostic problem. In each example, our model averaging approach provides better out-of-sample predictive performance than any single model that might reasonably have been selected. Abstract We consider the problem of model selection and accounting for model uncertainty in high-dimensional contingency tables, motivated by expert system applications. The approach most used currently is a stepwise strategy guided by tests based on approximate asymptotic P values leading to the selection of a single model; inference is then conditional on the selected model. The sampling properties of such a strategy are complex, and the failure to take account of model uncertainty leads to underestimation of uncertainty about quantities of interest. In principle, a panacea is provided by the standard Bayesian formalism that averages the posterior distributions of the quantity of interest under each of the models, weighted by their posterior model probabilities. Furthermore, this approach is optimal in the sense of maximizing predictive ability. But this has not been used in practice, because computing the posterior model probabilities is hard and the number of models is very large (often greater than 1011). We argue that the standard Bayesian formalism is unsatisfactory and propose an alternative Bayesian approach that, we contend, takes full account of the true model uncertainty by averaging over a much smaller set of models. An efficient search algorithm is developed for finding these models. We consider two classes of graphical models that arise in expert systems: the recursive causal models and the decomposable log-linear models. For each of these, we develop efficient ways of computing exact Bayes factors and hence posterior model probabilities. For the decomposable log-linear models, this is based on properties of chordal graphs and hyper-Markov prior distributions and the resultant calculations can be carried out locally. The end product is an overall strategy for model selection and accounting for model uncertainty that searches efficiently through the very large classes of models involved. Three examples are given. The first two concern data sets that have been analyzed by several authors in the context of model selection. The third addresses a urological diagnostic problem. In each example, our model averaging approach provides better out-of-sample predictive performance than any single model that might reasonably have been selected.},
author = {Madigan, David and Raftery, Adrian E},
booktitle = {Journal of the American Statistical Association},
doi = {10.1080/01621459.1994.10476894},
isbn = {0162-1459},
issn = {0162-1459},
number = {428},
pages = {1535--1546},
title = {{Model Selection and Accounting for Model Uncertainty in Graphical Models Using Occam's Window}},
url = {http://www.tandfonline.com/doi/abs/10.1080/01621459.1994.10476894},
volume = {89},
year = {1994}
}
@article{Madigan1995a,
author = {Madigan, David and Raftery, Adrian E and Volinsky, Chris T and Hoeting, Jennifer a},
file = {:C$\backslash$:/Users/snowdj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Madigan et al. - 1995 - Bayesian Model Averaging.pdf:pdf},
title = {{Bayesian Model Averaging}},
year = {1995}
}
@article{Marcellino2007b,
author = {Marcellino, Massimiliano and Bocconi, Universit\`{a} and Schumacher, Christian},
keywords = {authors,business cycle,data,ect the views of,large factor models,midas,missing values,mixed-frequency,not necessarily re,nowcasting,personal opinions and does,the,this paper represents the},
number = {October},
pages = {1--45},
title = {{Factor nowcasting of German GDP with ragged-edge data : A model comparison using MIDAS projections}},
year = {2007}
}
@article{Marcellino2007c,
author = {Marcellino, Massimiliano and Schumacher, Christian},
file = {:E$\backslash$:/Dropbox/phd/bsts/dynamicfator/factor/Factor-MIDAS for now- and forecasting.pdf:pdf},
keywords = {MIDAS, large factor models, nowcasting, mixed-freq},
number = {34},
title = {{Factor-MIDAS for now- and forecasting with ragged-edge data : a model comparison for German GDP Discussion Paper Series 1 : Economic Studies}},
year = {2007}
}
@article{Marcellino2010,
abstract = {This paper compares different ways to estimate the current state of the economy using factor models that can handle unbalanced datasets. Due to the different release lags of business cycle indicators, data unbalancedness often emerges at the end of multivariate samples, which is sometimes referred to as the 'ragged edge' of the data. Using a large monthly dataset of the German economy, we compare the performance of different factor models in the presence of the ragged edge: static and dynamic principal components based on realigned data, the Expectation-Maximisation (EM) algorithm and the Kalman smoother in a state-space model context. The monthly factors are used to estimate current quarter GDP, called the 'nowcast', using different versions of what we call factor-based mixed-data sampling (Factor-MIDAS) approaches. We compare all possible combinations of factor estimation methods and Factor-MIDAS projections with respect to nowcast performance. Additionally, we compare the performance of the nowcast factor models with the performance of quarterly factor models based on time-aggregated and thus balanced data, which neglect the most timely observations of business cycle indicators at the end of the sample. Our empirical findings show that the factor estimation methods don't differ much with respect to nowcasting accuracy. Concerning the projections, the most parsimonious MIDAS projection performs best overall. Finally, quarterly models are in general outperformed by the nowcast factor models that can exploit ragged-edge data.},
author = {Marcellino, Massimiliano and Schumacher, Christian},
doi = {10.1111/j.1468-0084.2010.00591.x},
file = {:E$\backslash$:/Dropbox/phd/bsts/dynamicfator/Factor MIDAS for Nowcasting and Forecasting.pdf:pdf},
isbn = {0305-9049},
issn = {03059049},
journal = {Oxford Bulletin of Economics and Statistics},
number = {4},
pages = {518--550},
title = {{Factor MIDAS for nowcasting and forecasting with ragged-edge data: A model comparison for German GDP}},
volume = {72},
year = {2010}
}
@article{Marcellino2006,
abstract = {“Iterated” multiperiod-ahead time series forecasts are made using a one-period ahead model, iterated forward for the desired number of periods, whereas “direct” forecasts are made using a horizon-specific estimated model, where the dependent variable is the multiperiod ahead value being forecasted. Which approach is better is an empirical matter: in theory, iterated forecasts are more efficient if the one-period ahead model is correctly specified, but direct forecasts are more robust to model misspecification. This paper compares empirical iterated and direct forecasts from linear univariate and bivariate models by applying simulated out-of-sample methods to 170 U.S. monthly macroeconomic time series spanning 1959–2002. The iterated forecasts typically outperform the direct forecasts, particularly, if the models can select long-lag specifications. The relative performance of the iterated forecasts improves with the forecast horizon.},
author = {Marcellino, Massimiliano and Stock, James H. and Watson, Mark W.},
doi = {10.1016/j.jeconom.2005.07.020},
file = {:C$\backslash$:/Users/snowdj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Marcellino, Stock, Watson - 2006 - A comparison of direct and iterated multistep AR methods for forecasting macroeconomic time series.pdf:pdf},
issn = {03044076},
journal = {Journal of Econometrics},
keywords = {C32,E37,E47,Forecast comparisons,Multistep forecasts,Var forecasts},
month = nov,
number = {1-2},
pages = {499--526},
title = {{A comparison of direct and iterated multistep AR methods for forecasting macroeconomic time series}},
url = {http://www.sciencedirect.com/science/article/pii/S030440760500165X},
volume = {135},
year = {2006}
}
@article{Mariano2010,
author = {Mariano, Roberto S and Murasawa, Yasutomo},
doi = {10.1111/j.1468-0084.2009.00567.x},
issn = {03059049},
journal = {Oxford Bulletin of Economics and Statistics},
number = {1},
pages = {27--46},
title = {{A coincident index, common factors, and monthly real GDP}},
volume = {72},
year = {2010}
}
@article{Matter2009b,
author = {Marsilli, Cl\'{e}ment},
file = {:E$\backslash$:/Dropbox/phd/MIDAS/variableselection/c\_marsilli\_Nov\_2014.pdf:pdf},
number = {December},
title = {{VARIABLE SELECTION IN PREDICTIVE MIDAS MODELS}},
year = {2014}
}
@article{Matter2009,
author = {Matter, Wealth},
number = {December},
title = {{Document De Travail}},
year = {2009}
}
@book{Mikosch2015,
author = {Mikosch, Heiner and Neuwirth, Stefan},
isbn = {9789523230422},
keywords = {E27 Keywords: forecastin,JEL classi cations: C53},
title = {{Real-time forecasting with a MIDAS VAR Institute for Economies in Transition}},
year = {2015}
}
@article{Mitchell2014,
author = {Mitchell, James},
doi = {10.1016/j.ijforecast.2013.11.002},
file = {:C$\backslash$:/Users/snowdj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mitchell - 2014 - Discussion of “Forecasting macroeconomic variables using collapsed dynamic factor analysis” by Falk Br\"{a}uning and Sie.pdf:pdf},
issn = {01692070},
journal = {International Journal of Forecasting},
month = jul,
number = {3},
pages = {585--588},
title = {{Discussion of “Forecasting macroeconomic variables using collapsed dynamic factor analysis” by Falk Br\"{a}uning and Siem Jan Koopman}},
url = {http://www.sciencedirect.com/science/article/pii/S0169207013001611},
volume = {30},
year = {2014}
}
@article{Mol2006,
author = {Mol, Christine De and Giannone, Domenico and Reichlin, Lucrezia},
keywords = {Bayesian VAR,Lasso regression,ridge regression},
number = {700},
title = {{Wo R K I N G Pa P E R S E R I E S No 700 / December 2006 Forecasting Using a Large Number of Predictors Is Bayesian Regression a Valid Alternative To Principal Components ? Wo R K I N G Pa P E R S E R I E S a Large Number of Predictors Is Bayesian Regress}},
year = {2006}
}
@article{Mourougane2006,
abstract = {The objective of this paper is to develop a short-term indicator-based model to predict quarterly GDP in Canada by efficiently exploiting all available monthly information. To this aim, monthly forecasting equations are estimated using the GDP series published every month by Statistics Canada as well as other monthly indicators. The procedures are automated and the model can be run whenever major monthly data are released, allowing the appropriate choice of the model according to the information set available. The most important gain from this procedure is for the current-quarter forecast when one or two months of GDP data are available, with all monthly models estimated in the paper outperforming a standard quarterly autoregressive model in terms of size of errors. The use of indicators also appears to improve forecasting performance, especially when an average of indicator-based models is used. Real-time forecasting performance of the average model appear to be good, with an apparent stability of the estimates from one update to the next, despite the extensive use of monthly data. The latter result should nonetheless be interpreted with caution and will need to be re-assessed when more data become available.},
author = {Mourougane, Annabelle},
file = {:E$\backslash$:/Dropbox/phd/forecast/Canada/Forecasting Monthly GDP For Canada.pdf:pdf},
number = {515},
title = {{Forecasting monthly GDP for Canada}},
url = {http://ideas.repec.org/p/oec/ecoaaa/515-en.html},
year = {2006}
}
@article{Ng2013,
abstract = {The object of this paper is to produce non-parametric maximum likelihood estimates of forecast distributions in a general non-Gaussian, non-linear state space setting. The transition densities that define the evolution of the dynamic state process are represented in parametric form, but the conditional distribution of the non-Gaussian variable is estimated non-parametrically. The filtered and prediction distributions are estimated via a computationally efficient algorithm that exploits the functional relationship between the observed variable, the state variable and a measurement error with an invariant distribution. Simulation experiments are used to document the accuracy of the non-parametric method relative to both correctly and incorrectly specified parametric alternatives. In an empirical illustration, the method is used to produce sequential estimates of the forecast distribution of realized volatility on the S\&P500 stock index during the recent financial crisis. A resampling technique for measuring sampling variation in the estimated forecast distributions is also demonstrated.},
author = {Ng, Jason and Forbes, Catherine S and Martin, Gael M and McCabe, Brendan P M},
doi = {10.1016/j.ijforecast.2012.10.005},
issn = {01692070},
journal = {International Journal of Forecasting},
keywords = {Grid-based filtering,Non-Gaussian time series,Penalized likelihood,Probabilistic forecasting,Realized volatility,Subsampling},
month = jul,
number = {3},
pages = {411--430},
title = {{Non-parametric estimation of forecast distributions in non-Gaussian, non-linear state space models}},
url = {http://www.sciencedirect.com/science/article/pii/S0169207012001665},
volume = {29},
year = {2013}
}
@article{Ng2013a,
abstract = {This paper provides a survey of business cycle facts, updated to take account of recent data. Emphasis is given to the Great Recession, which was unlike most other postwar recessions in the United States in being driven by deleveraging and financial market factors. We document how recessions with financial market origins are different from those driven by supply or monetary policy shocks. This helps explain why economic models and predictors that work well at some times do poorly at other times. We discuss challenges for forecasters and empirical researchers in light of the updated business cycle facts.},
author = {Ng, Serena and Wright, Jonathan H.},
doi = {10.1257/jel.51.4.1120},
file = {:E$\backslash$:/Dropbox/phd/forecast/Facts and Challenges from the Great Recession for.pdf:pdf},
issn = {0022-0515},
journal = {Journal of Economic Literature},
number = {4},
pages = {1120--1154},
title = {{Facts and Challenges from the Great Recession for Forecasting and Macroeconomic Modeling}},
volume = {51},
year = {2013}
}
@article{No2013,
author = {No, Paper},
title = {{Australian School of Business Working Paper Forecasting using a large number of predictors : Bayesian model averaging versus principal components regression}},
year = {2013}
}
@article{Nowman1998,
author = {Nowman, K.B.},
doi = {10.1023/A:1008682814171},
issn = {1572-9974},
journal = {Computational Economics},
language = {en},
month = dec,
number = {3},
pages = {243--254},
publisher = {Kluwer Academic Publishers},
title = {{Econometric Estimation of a Continuous Time Macroeconomic Model of the United Kingdom with Segmented Trends}},
url = {http://link.springer.com/article/10.1023/A:1008682814171},
volume = {12},
year = {1998}
}
@article{Ortega2014,
author = {Ortega, Juan-pablo and Bauwens, Luc},
title = {{Mixed-frequency modeling and economic forecasting}},
year = {2014}
}
@article{Ouysse2013,
author = {Ouysse, Rachida},
file = {:C$\backslash$:/Users/snowdj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ouysse - 2013 - Forecasting using a large number of predictors Bayesian model averaging versus principal components regression.pdf:pdf},
title = {{Forecasting using a large number of predictors: Bayesian model averaging versus principal components regression}},
year = {2013}
}
@book{Petris2008,
author = {Petris, Giovanni and Petrone, Sonia and Campagnoli, Patrizia},
isbn = {978-0-387-77237-0},
pages = {252},
title = {{Dynamic Linear Models with R}},
url = {http://www.springer.com/statistics/statistical+theory+and+methods/book/978-0-387-77237-0},
year = {2008}
}
@article{Poncela2004,
author = {Poncela, Pilar},
doi = {10.1016/j.ijforecast.2003.11.005},
isbn = {0198523548 (acid-free paper)},
issn = {01692070},
journal = {International Journal of Forecasting},
number = {1},
pages = {139--141},
title = {{Time series analysis by state space methods}},
volume = {20},
year = {2004}
}
@article{Radchenko2006,
abstract = {Using Markov Chain Monte Carlo algorithms within the limited information Bayesian framework, we estimate the parameters of the structural equation of interest and test weak exogeneity in a simultaneous equation model with white noise as well as autocorrelated error terms. A numerical example and an estimation of the supply and demand equations of the U.S. gasoline market show that if we ignore autocorrelation we obtain unreasonable posterior distributions of the parameters of interest. Also we find that the hypothesis of the asymmetric effect of the changes in oil price on the changes in gasoline price is rejected. Oil inventory has a significant negative effect on the gasoline price.},
author = {Radchenko, Stanislav and Tsurumi, Hiroki},
doi = {10.1016/j.jeconom.2005.03.008},
issn = {03044076},
journal = {Journal of Econometrics},
month = jul,
number = {1},
pages = {31--49},
title = {{Limited information Bayesian analysis of a simultaneous equation with an autocorrelated error term and its application to the U.S. gasoline market}},
url = {http://www.sciencedirect.com/science/article/pii/S0304407605000825},
volume = {133},
year = {2006}
}
@article{Rodriguez2010,
abstract = {We describe Bayesian models for economic and financial time series that use regressors sampled at higher frequencies than the outcome of interest. The models are developed within the framework of dynamic linear models, which provides a high level of flexibility and allows direct interpretation of the results. The problem of the collinearity of intraperiod observations is solved using model selection and model averaging approaches. Bayesian approaches to model selection automatically adjust for multiple comparisons, while predictions based on model averaging allow us to account for both model and parameter uncertainty when predicting future observations. A novel aspect of the models presented here is the introduction of new formulations for the prior distribution on the model space that allow us to favor sparse models where the significant coefficients cluster on adjacent lags of the high frequency predictor. We illustrate our approach by predicting the gross national product of the United States using the term structure of interest rates.},
author = {Rodriguez, Abel and Puggioni, Gavino},
doi = {10.1016/j.ijforecast.2010.01.009},
file = {:C$\backslash$:/Users/snowdj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Rodriguez, Puggioni - 2010 - Mixed frequency models Bayesian approaches to estimation and prediction.pdf:pdf},
issn = {01692070},
journal = {International Journal of Forecasting},
keywords = {Gross national product,Interest rates,Mixed frequency data,Model averaging,Model selection},
month = apr,
number = {2},
pages = {293--311},
title = {{Mixed frequency models: Bayesian approaches to estimation and prediction}},
url = {http://www.sciencedirect.com/science/article/pii/S0169207010000154},
volume = {26},
year = {2010}
}
@article{Schorfheide2012,
abstract = {This paper develops a vector autoregression (VAR) for macroeconomic time series which are observed at mixed frequencies – quarterly and monthly. The mixed-frequency VAR is cast in state-space form and estimated with Bayesian methods under a Minnesota-style prior. Using a real-time data set, we generate and evaluate forecasts from the mixed-frequency VAR and compare them to forecasts from a VAR that is estimated based on data time-aggregated to quarterly frequency. We document how information that becomes available within the quarter improves the forecasts in real time.},
author = {Schorfheide, Frank and Song, Dongho},
file = {:C$\backslash$:/Users/snowdj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Schorfheide, Song - 2012 - Real-time forecasting with a mixed-frequency VAR.pdf:pdf},
journal = {Working Papers},
keywords = {Bayesian statistical decision theory,Forecasting,Vector autoregression},
publisher = {Federal Reserve Bank of Minneapolis},
title = {{Real-time forecasting with a mixed-frequency VAR}},
url = {http://ideas.repec.org/p/fip/fedmwp/701.html},
year = {2012}
}
@article{Schumacher2014,
author = {Schumacher, Christian},
file = {:E$\backslash$:/Dropbox/phd/MIDAS/bridgeequation/MIDAS and bridge equations.pdf:pdf},
title = {{MIDAS and bridge equations}},
year = {2014}
}
@article{Scott2014,
abstract = {We consider the problem of short-term time series forecasting (nowcasting) when there are more possible predictors than observations. The motivating example is the use of Google Trends search engine query data as a contemporaneous predictor of economic indicators. Our preferred approach combines three Bayesian techniques: Kalman filtering, spike-and-slab regression, and model averaging. The Kalman filter can be used to control for time series feature, such as seasonality and trend; the regression can be used to incorporate predictors such as search engine queries; and model averaging can be used to reduce the danger of overfitting. Overall the Bayesian approach allows a flexible way to incorporate prior knowledge, both subjective and objective, into the estimation procedure. We illustrate this approach using search engine query data as predictors for consumer sentiment and gun sales.},
author = {Scott, Steven L. and Varian, Hal R.},
file = {:E$\backslash$:/Dropbox/phd/bsts/fat.pdf:pdf},
journal = {Economics of Digitization},
pages = {1--22},
title = {{Bayesian Variable Selection for Nowcasting Economic Time Series}},
url = {http://www.nber.org/papers/w19567.pdf},
year = {2014}
}
@article{Scott2013,
abstract = {We consider the problem of short-term time series forecasting (nowcasting) when there are more possible predictors than observations. Our approach combines three Bayesian techniques: Kalman filtering, spike-and-slab regression, and model averaging. We illustrate this approach using search engine query data as predictors for consumer sentiment and gun sales.},
author = {Scott, Steven L. and Varian, Hal R.},
title = {{NBER WORKING PAPER SERIES BAYESIAN VARIABLE SELECTION FOR NOWCASTING ECONOMIC TIME SERIES Bayesian Variable Selection for Nowcasting Economic Time Series}},
url = {http://www.nber.org/papers/w19567},
year = {2013}
}
@article{Scott2014a,
abstract = {This article describes a system for short term forecasting based on an ensemble prediction that averages over di erent combinations of predictors. The system combines a structural time series model for the target series with regression component capturing the contributions of contemporaneous search query data. A spike-and-slab prior on the regression coecients induces sparsity, dramatically reducing the size of the regression problem. Our system averages over potential contributions from a very large set of models and gives easily digested reports of which coecients are likely to be important. We illustrate with applications to initial claims for unemployment bene ts and to retail sales. Although our exposition focuses on using search engine data to forecast economic time series, the underlying statistical methods can be applied to more general short term forecasting with large numbers of contemporaneous predictors.},
author = {Scott, Steven L. and Varian, Hal R.},
doi = {10.1504/IJMMNO.2014.059942},
file = {:E$\backslash$:/Dropbox/phd/bsts/bsts-2014.pdf:pdf},
issn = {2040-3607},
journal = {International Journal of Mathematical Modelling and Numerical Optimisation},
keywords = {Bayesian methods,forecasting,model selection},
number = {1/2},
pages = {4},
title = {{Predicting the present with Bayesian structural time series}},
url = {http://www.inderscience.com/storage/f581249103271116.pdf},
volume = {5},
year = {2014}
}
@article{Sells1995,
abstract = {A stochastic dynamic programme determines the farmer's long-term weed control strategy incorporating decisions of crop, autumn cultivations, timing of planting and herbicide treatments. A previous paper (Sells, (1993) Agric. Syst., 41, 41–52) describes the one-step transition probabilities due to variable weed control from year to year. This paper describes the costing of the decision options and shows the results from the model for the optimal control of one difficult grass weed in cereals. The model is shown to give reasonable results and shows that the control strategy which optimises costs is to use half-rate herbicide over a wide range of seedbanks. A simulation of the optimum strategy over 10 years shows that the total overall use of herbicide is also reduced.},
author = {Sells, J.E. E},
doi = {10.1016/0308-521X(94)00016-K},
file = {:C$\backslash$:/Users/snowdj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sells - 1995 - Optimising weed management using stochastic dynamic programming to take account of uncertain herbicide performance.pdf:pdf},
issn = {0308521X},
journal = {Agricultural Systems},
month = jan,
number = {3},
pages = {271--296},
title = {{Optimising weed management using stochastic dynamic programming to take account of uncertain herbicide performance}},
url = {http://www.sciencedirect.com/science/article/pii/0308521X9400016K},
volume = {48},
year = {1995}
}
@article{Sells1993,
abstract = {A dynamic programme to optimise the farmer's long-term weed management problem is described in general terms incoroprating decisions of crop, autumn cultivations, timing of planting (winter and spring crops) and herbicide use. This paper concentrates on the formulation of one-step transition probabilities given the uncertain nature of herbicide performance. For a model considering one weed in a cereal rotation examples of the one-step transition probability matrices are given for wild oat control with and without a herbicide application.},
author = {Sells, J.E. E},
doi = {10.1016/0308-521X(93)90080-L},
file = {:C$\backslash$:/Users/snowdj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sells - 1993 - Calculating transition probabilities for modelling weed management options using stochastic dynamic programming.pdf:pdf},
issn = {0308521X},
journal = {Agricultural Systems},
month = jan,
number = {1},
pages = {41--52},
title = {{Calculating transition probabilities for modelling weed management options using stochastic dynamic programming}},
url = {http://www.sciencedirect.com/science/article/pii/0308521X9390080L},
volume = {41},
year = {1993}
}
@article{Semieniuk2014,
author = {Semieniuk, Gregor and Treeck, Till Van and Truger, Achim},
isbn = {4921177783},
number = {October 2011},
title = {{Working Paper}},
year = {2014}
}
@article{Uhl2011,
abstract = {In this paper we extend the targeted-regressor approach suggested in Bai and Ng (2008) for variables sampled at the same frequency to mixed-frequency data. Our MIDASSO approach is a combination of the unrestricted MIxed-frequency DAta-Sampling approach (U-MIDAS) (see Foroni et al., 2015; Castle et al., 2009; Bec and Mogliani, 2013), and the LASSO-type penalised regression used in Bai and Ng (2008), called the elastic net (Zou and Hastie, 2005). We illustrate our approach by forecasting the quarterly real GDP growth rate in Switzerland.},
author = {Siliverstovs, Boriss},
file = {:E$\backslash$:/Dropbox/phd/forecast/Short-term forecasting with mixed-frequency data A MIDASSO approach.pdf:pdf},
title = {{Short-term forecasting with mixed-frequency data: A MIDASSO approach}},
url = {http://www.kof.ethz.ch/en/publications/p/kof-working-papers/158/},
year = {2015}
}
@article{Stock2009,
abstract = {An ongoing theme in David Hendry’s work has been concern about detecting and avoiding forecast breakdowns that arise because of structural instability. Parameter instability can arise for various reasons, including structural breaks in the economy (for example, changes in technology), policy regime shifts, or changes in the survey instruments from which the time series are constructed. Hendry and coauthors have argued that such instability, whatever its source, often manifests itself as breaks in time series forecasting relations, and moreover that such breaks constitute one of the primary reasons for forecast failures in practice (see for example Clements and Hendry [1999, 2002], Hendry and Clements [2002], Hendry [2005], and Hendry and Mizon [2005]). One line of Hendry’s research has been to develop and to analyze non-structural forecasting methods for their potential robustness to parameter instability, including error correction models, overdifferencing, intercept shift methods, and – closest to the focus of this paper – forecast pooling (Hendry and Clements [2002]). This paper continues this line of inquiry, in which forecasting methods are examined for their reliability in the face of structural breaks. We focus here on forecasts constructed using dynamic factor models (DFMs; Geweke [1977], Sargent and Sims [1977]). In DFMs, the comovements of the observable time series are characterized by latent dynamic factors. Over the past decade, work on DFMs has focused on high- dimensional systems in which very many series depend on a handful of factors (Forni, Lippi, Hallin, and Reichlin [2000], Stock and Watson [2002a, 2002b], and many others; for a survey, see Stock and Watson [2006]). These factor-based forecasts have had notable empirical forecasting successes. Yet, there has been little work to date on the performance of factor-based macroeconomic forecasts under structural instability (exceptions are Stock and Watson (1998, 2002b) and Banerjee, Marcellino, and Masten (2007), which are discussed below).},
author = {Stock, J H and Watson, M W},
journal = {The Methodology and Practice of Econometrics. A Festschrift in Honour of David F. Hendry},
number = {August 2007},
pages = {173--205},
title = {{Forecasting in Dynamic Factor Models Subject To Structural Instability}},
url = {http://books.google.com/books?hl=en\&amp;lr=\&amp;id=-w66-4C0JDcC\&amp;oi=fnd\&amp;pg=PA173\&amp;dq=FORECASTING+IN+DYNAMIC+FACTOR+MODELS+SUBJECT+TO+STRUCTURAL+INSTABILITY\&amp;ots=bcvHlMU9bN\&amp;sig=Y-SvteVpCVvtb5QV-sAU2-dS-2o},
year = {2009}
}
@article{Stock2005a,
abstract = {This paper considers VAR models incorporating many time series that interact through a few dynamic factors. Several econometric issues are addressed including estimation of the number of dynamic factors and tests for the factor restrictions imposed on the VAR. Structural VAR identification based on timing restrictions, long run restrictions, and restrictions on factor loadings are discussed and practical computational methods suggested. Empirical analysis using U.S. data suggest several (7) dynamic factors, rejection of the exact dynamic factor model but support for an approximate factor model, and sensible results for a SVAR that identifies money policy shocks using timing restrictions.},
author = {Stock, J.H. and Watson, M.W.},
file = {:E$\backslash$:/Dropbox/phd/statespace/dfm/dfmdata/favar.pdf:pdf},
journal = {NBER working paper},
number = {June},
title = {{Implications of Dynamic Factor Models}},
url = {http://papers.ssrn.com/sol3/papers.cfm?abstract\_id=755703},
year = {2005}
}
@article{Stocka,
author = {Stock, James H},
file = {:C$\backslash$:/Users/snowdj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Stock - Unknown - Forecasting and Now-Casting with Disparate Predictors Dynamic Factor Models and Beyond FEMES 2006 Meetings Beijing.pdf:pdf},
title = {{Forecasting and Now-Casting with Disparate Predictors: Dynamic Factor Models and Beyond FEMES 2006 Meetings Beijing}}
}
@article{Stock2004,
abstract = {This paper provides a simple shrinkage representation that describes the operational characteristics of various forecasting methods that are applicable when there are a large number of orthogonal predictors (such as principal components). These methods include pretest methods, Bayesian model averaging, empirical Bayes, and bagging. We then compare these and other many-predictor forecasting methods in the context of macroeconomic forecasting (real activity and inflation) using 131 monthly predictors with monthly U.S. economic time series data, 1959:1 -2003:12. The theoretical shrinkage representations serve to inform our empirical comparison of these forecasting methods.},
author = {Stock, James H and Watson, Mark W and Thank, We and Boivin, Jean and Giannone, Domenico and Kilian, Lutz and Ng, Serena and Reichlin, Lucrezia and Steele, Mark and Wright, Jonathan},
title = {{AN EMPIRICAL COMPARISON OF METHODS FOR FORECASTING USING MANY PREDICTORS}},
year = {2004}
}
@article{Stock2012b,
abstract = {This paper provides a simple shrinkage representation that describes the operational characteristics of various forecasting methods designed for a large number of orthogonal predictors (such as principal components). These methods include pretest methods, Bayesian model averaging, empirical Bayes, and bagging. We compare empirically forecasts from these methods to dynamic factor model (DFM) forecasts using a U.S. macroeconomic data set with 143 quarterly variables spanning 1960-2008. For most series, including measures of real economic activity, the shrinkage forecasts are inferior to the DFM forecasts},
author = {Stock, Jh and Watson, Mw},
doi = {10.1080/07350015.2012.715956},
file = {:E$\backslash$:/Dropbox/phd/statespace/dfm/dfmdata/stock\_watson\_generalized\_shrinkage\_supplement\_June\_2012.pdf:pdf},
isbn = {0735-0015$\backslash$r1537-2707},
issn = {0735-0015},
journal = {Journal of Business and Economic Statistics},
keywords = {dynamic factor models,empirical Bayes,high dimensional model},
number = {4},
pages = {481--493},
title = {{Generalized shrinkage methods for forecasting using many predictors}},
url = {http://amstat.tandfonline.com/doi/abs/10.1080/07350015.2012.715956},
volume = {30},
year = {2012}
}
@article{Stock2006,
author = {Stock, Jh and Watson, Mw},
file = {:E$\backslash$:/Dropbox/phd/bsts/dynamicfator/factor/dfm\_oup\_4.pdf:pdf},
isbn = {9780195398649},
journal = {Oxford Handbook of Economic Forecasting},
number = {January},
pages = {1--43},
title = {{Dynamic factor models}},
url = {http://link.springer.com/article/10.1007/s10182-006-0219-z},
year = {2006}
}
@article{Structural2015,
author = {Structural, Title and Series, Time},
title = {{Package ‘ stsm ’}},
year = {2015}
}
@article{Taieb2014,
abstract = {Multi-step forecasts can be produced recursively by iterating a one-step model, or directly using a specific model for each horizon. Choosing between these two strategies is not an easy task since it involves a trade-off between bias and estimation variance over the forecast horizon. Using a nonlinear machine learning model makes the tradeoff even more difficult. To address this issue, we propose a new forecasting strategy which boosts traditional recursive linear forecasts with a direct strategy using a boosting autoregression procedure at each horizon. First, we investigate the performance of the proposed strategy in terms of bias and variance decomposition of the error using simulated time series. Then, we evaluate the proposed strategy on real-world time series from two forecasting competitions. Overall, we obtain excellent performance with respect to the standard forecasting strategies.},
author = {Taieb, Souhaib Ben and Hyndman, Rob J},
journal = {Monash Econometrics and Business Statistics Working Papers},
keywords = {Multi-step forecasting,boosting,direct forecasting,forecasting strategies,linear time series,nonlinear time series,recursive forecasting},
publisher = {Monash University, Department of Econometrics and Business Statistics},
title = {{Boosting multi-step autoregressive forecasts}},
url = {http://ideas.repec.org/p/msh/ebswps/2014-13.html},
year = {2014}
}
@misc{TheMendeleySupportTeam2011,
abstract = {A quick introduction to Mendeley. Learn how Mendeley creates your personal digital library, how to organize and annotate documents, how to collaborate and share with colleagues, and how to generate citations and bibliographies.},
address = {London},
author = {{The Mendeley Support Team}},
booktitle = {Mendeley Desktop},
keywords = {Mendeley,how-to,user manual},
pages = {1--16},
publisher = {Mendeley Ltd.},
title = {{Getting Started with Mendeley}},
url = {http://www.mendeley.com},
year = {2011}
}
@misc{Tibshirani1994,
abstract = {Document: Details (1994) Robert Tibshirani CiteSeer.IST - Copyright Penn State and NEC},
author = {Tibshirani, Robert},
booktitle = {Journal of the Royal Statistical Society B},
doi = {10.2307/2346178},
isbn = {0849320240},
issn = {00359246},
pages = {267--288},
pmid = {16272381},
title = {{Regression Selection and Shrinkage via the Lasso}},
url = {http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.35.7574},
volume = {58},
year = {1994}
}
@incollection{Timmermann2006,
abstract = {Forecast combinations have frequently been found in empirical studies to produce better forecasts on average than methods based on the ex ante best individual forecasting model. Moreover, simple combinations that ignore correlations between forecast errors often dominate more refined combination schemes aimed at estimating the theoretically optimal combination weights. In this chapter we analyze theoretically the factors that determine the advantages from combining forecasts (for example, the degree of correlation between forecast errors and the relative size of the individual models' forecast error variances). Although the reasons for the success of simple combination schemes are poorly understood, we discuss several possibilities related to model misspecification, instability (non-stationarities) and estimation error in situations where the number of models is large relative to the available sample size. We discuss the role of combinations under asymmetric loss and consider combinations of point, interval and probability forecasts.},
author = {Timmermann, Allan},
booktitle = {Handbook of Economic Forecasting},
editor = {Elliott, Graham and Granger., Clive W.J. and Timmermann, Allan},
pages = {135--196},
publisher = {Elsevier},
title = {{Forecast Combinations}},
url = {http://econpapers.repec.org/RePEc:eee:ecofch:1-04},
volume = {1},
year = {2006}
}
@article{Tkacz2001,
abstract = {The objective of this paper is to improve the accuracy of financial and monetary forecasts of Canadian output growth by using leading indicator neural network models. We find that neural networks yield statistically lower forecast errors for the year-over-year growth rate of real GDP relative to linear and univariate models. However, such forecast improvements are less notable when forecasting quarterly real GDP growth. Neural networks are unable to outperform a naive no-change model. More pronounced non-linearities at the longer horizon is consistent with the possible asymmetric effects of monetary policy on the real economy. ?? International Institute of Forecasters.},
author = {Tkacz, Greg},
doi = {10.1016/S0169-2070(00)00063-7},
issn = {01692070},
journal = {International Journal of Forecasting},
keywords = {Artificial neural networks,Backpropagation,Business cycle indicators,Comparative methods,Economic policy,Macroeconomic forecasting,Macroeconomic indicators},
number = {1},
pages = {57--69},
title = {{Neural network forecasting of Canadian GDP growth}},
volume = {17},
year = {2001}
}
@article{Toda1995,
abstract = {This paper shows how we can estimate VAR's formulated in levels and test general restrictions on the parameter matrices even if the processes may be integrated or cointegrated of an arbitrary order. We can apply a usual lag selection procedure to a possibly integrated or cointegrated VAR since the standard asymptotic theory is valid (as far as the order of integration of the process does not exceed the true lag length of the model). Having determined a lag length k, we then estimate a (k + dmax)th-order VAR where dmax is the maximal order of integration that we suspect might occur in the process. The coefficient matrices of the last dmax lagged vectors in the model are ignored (since these are regarded as zeros), and we can test linear or nonlinear restrictions on the first k coefficient matrices using the standard asymptotic theory.},
author = {Toda, Hiro Y. and Yamamoto, Taku},
doi = {10.1016/0304-4076(94)01616-8},
file = {:C$\backslash$:/Users/snowdj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Toda, Yamamoto - 1995 - Statistical inference in vector autoregressions with possibly integrated processes.pdf:pdf},
issn = {03044076},
journal = {Journal of Econometrics},
keywords = {C32,Cointegration,Hypothesis testing,Lag order selection,Unit roots,VAR test for Granger causality,Vector autoregressions},
mendeley-tags = {VAR test for Granger causality},
month = mar,
number = {1-2},
pages = {225--250},
title = {{Statistical inference in vector autoregressions with possibly integrated processes}},
url = {http://www.sciencedirect.com/science/article/pii/0304407694016168},
volume = {66},
year = {1995}
}
@article{Tusell2011,
abstract = {Support in R for state space estimation viaKalman filtering was limited to one package, until fairly recently. In the last five years, the situation has changed with no less than four additional packages offering general implementations of theKalman filter, including in some cases smoothing, simulation smoothing and other functionality. This paper reviews some of the offerings in R to help the prospective user to make an informed choice.},
author = {Tusell, Fernando},
issn = {15487660},
journal = {Journal of Statistical Software},
keywords = {kalman filter,r,state space models,time series},
number = {2},
pages = {1--27},
pmid = {18291371},
title = {{Journal of Statistical Software}},
url = {http://www.jstatsoft.org/v39/i02/paper},
volume = {39},
year = {2011}
}
@misc{Tversky1992,
abstract = {We develop a new version of prospect theory that employs cumulative rather than separable decision weights and extends the theory in several respects. This version, called cumulative prospect theory, applies to uncertain as well as to risky prospects with any number of outcomes, and it allows different weighting functions for gains and for losses. Two principles, diminishing sensitivity and loss aversion, are invoked to explain the characteris- tic curvature of the value function and the weighting functions. A review of the experimental evidence and the results of a new experiment confirm a distinctive fourfold pattern of risk attitudes: risk aversion for gains and risk seeking for losses of high probability; risk seeking for gains and risk aversion for losses of low probability. Expected},
author = {Tversky, Amos and Kahneman, Daniel},
booktitle = {Journal of Risk and Uncertainty},
doi = {10.1007/BF00122574},
isbn = {0521627494},
issn = {0895-5646},
number = {4},
pages = {297--323},
pmid = {15795132},
title = {{Advances in prospect theory: Cumulative representation of uncertainty}},
volume = {5},
year = {1992}
}
@article{VanOudenhoven2002,
abstract = {The present study considered the reliability and validity of the 78-item revised version of the Multicultural Personality Questionnaire, a multidimensional instrument aimed at measuring multicultural effectiveness of expatriate employees and students. The questionnaire includes scales for cultural empathy, open-mindedness, emotional stability, social initiative and flexibility. Participants were native and foreign students of an international business school (N=171) in the Netherlands. The MPQ scales appeared to be more strongly predictive of adjustment of international students as compared to native students. Moreover, the instrument was able to explain variance in students’ adjustment beyond self-efficacy.},
author = {{Van Oudenhoven}, Jan Pieter and {Van der Zee}, Karen I},
doi = {10.1016/S0147-1767(02)00041-X},
issn = {01471767},
journal = {International Journal of Intercultural Relations},
keywords = {Cultural empathy,Emotional stability,Flexibility,International students,Multicultural Personality Questionnaire,Multicultural effectiveness,Open-mindedness,Social Initiative},
month = nov,
number = {6},
pages = {679--694},
title = {{Predicting multicultural effectiveness of international students: the Multicultural Personality Questionnaire}},
url = {http://www.sciencedirect.com/science/article/pii/S014717670200041X},
volume = {26},
year = {2002}
}
@article{Vanhoucke2004,
author = {Vanhoucke, Mario},
isbn = {9789251081518},
number = {January},
title = {{Working Paper}},
year = {2004}
}
@article{Varian,
abstract = {C C omputers are now involved in many economic transactions and can capture omputers are now involved in many economic transactions and can capture data associated with these transactions, which can then be manipulated data associated with these transactions, which can then be manipulated and analyzed. Conventional statistical and econometric techniques such and analyzed. Conventional statistical and econometric techniques such as regression often work well, but there are issues unique to big datasets that may as regression often work well, but there are issues unique to big datasets that may require different tools. require different tools. First, the sheer size of the data involved may require more powerful data First, the sheer size of the data involved may require more powerful data manipulation tools. Second, we may have more potential predictors than appro-manipulation tools. Second, we may have more potential predictors than appro-priate for estimation, so we need to do some kind of variable selection. Third, priate for estimation, so we need to do some kind of variable selection. Third, large datasets may allow for more fl exible relationships than simple linear models. large datasets may allow for more fl exible relationships than simple linear models. Machine learning techniques such as decision trees, support vector machines, Machine learning techniques such as decision trees, support vector machines, neural nets, deep learning, and so on may allow for more effective ways to model neural nets, deep learning, and so on may allow for more effective ways to model complex relationships. complex relationships. In this essay, I will describe a few of these tools for manipulating and analyzing In this essay, I will describe a few of these tools for manipulating and analyzing big data. I believe that these methods have a lot to offer and should be more widely big data. I believe that these methods have a lot to offer and should be more widely known and used by economists. In fact, my standard advice to graduate students known and used by economists. In fact, my standard advice to graduate students these days is go to the computer science department and take a class in machine these days is go to the computer science department and take a class in machine learning. There have been very fruitful collaborations between computer scien-learning. There have been very fruitful collaborations between computer scien-tists and statisticians in the last decade or so, and I expect collaborations between tists and statisticians in the last decade or so, and I expect collaborations between computer scientists and econometricians will also be productive in the future. computer scientists and econometricians will also be productive in the future.},
author = {Varian, Hal R},
doi = {10.1257/jep.28.2.3},
journal = {Journal of Economic Perspectives—Volume},
number = {2},
pages = {2013--2014},
title = {{Big Data: New Tricks for Econometrics Tools to Manipulate Big Data}},
url = {http://dx.doi.org/10.1257/jep.28.2.3},
volume = {28}
}
@article{West2013,
abstract = {Since the 1970s, applications of Bayesian time series models and forecasting methods have represented major success stories for our discipline. Dynamic modelling is a very broad field, so this ISBA Lecture on Bayesian Foundations will rather selectively note key concepts and some core model contexts, leavened with extracts froma few time series analysis and forecasting examples from various application fields. The Lecture with then link into and briefly discuss a range of recent developments in exciting and challenging areas of Bayesian time series analysis.},
author = {West, Mike},
doi = {10.1093/acprof:oso/9780199695607.003.0008},
isbn = {9780199695607},
journal = {Bayesian Inference and Markov Chain Monte Carlo: In \ldots},
title = {{Bayesian dynamic modelling}},
url = {http://books.google.com/books?hl=en\&lr=\&id=rpuo2eC6-EsC\&oi=fnd\&pg=PA145\&dq=Bayesian+dynamic+modelling\&ots=tmZ0q4kXbx\&sig=MQVb9RSpunR0GoJ7XEWrDvgZuyY},
year = {2013}
}
@article{Wohlrabe2014,
abstract = {The use of large datasets for macroeconomic forecasting has received a great deal of interest recently. Boosting is one possible method of using high-dimensional data for this purpose. It is a stage-wise additive modelling procedure, which, in a linear specification, becomes a variable selection device that iteratively adds the predictors with the largest contribution to the fit. Using data for the United States, the euro area and Germany, we assess the performance of boosting when forecasting a wide range of macroeconomic variables. Moreover, we analyse to what extent its forecasting accuracy depends on the method used for determining its key regularisation parameter, the number of iterations. We find that boosting mostly outperforms the autoregressive benchmark, and that \$K\$-fold cross-validation works much better as stopping criterion than the commonly used information criteria. (This abstract was borrowed from another version of this item.)},
author = {Wohlrabe, Klaus and Buchen, Teresa},
file = {:E$\backslash$:/Dropbox/phd/shrinkage/boosting/Assessing the Macroeconomic Forecasting Performance of Boosting Evidence for the United States, the Euro Area, and Germany.pdf:pdf},
journal = {Journal of Forecasting},
number = {4},
pages = {231--242},
publisher = {John Wiley \& Sons, Ltd.},
title = {{Assessing the Macroeconomic Forecasting Performance of Boosting: Evidence for the United States, the Euro Area and Germany}},
url = {http://ideas.repec.org/a/wly/jforec/v33y2014i4p231-242.html},
volume = {33},
year = {2014}
}
@book{Wooldridge2002,
abstract = {This graduate text provides an intuitive but rigorous treatment of contemporary methods used in microeconometric research. The book makes clear that applied microeconometrics is about the estimation of marginal and treatment effects, and that parametric estimation is simply a means to this end. It also clarifies the distinction between causality and statistical association.The book focuses specifically on cross section and panel data methods. Population assumptions are stated separately from sampling assumptions, leading to simple statements as well as to important insights. The unified approach to linear and nonlinear models and to cross section and panel data enables straightforward coverage of more advanced methods. The numerous end-of-chapter problems are an important component of the book. Some problems contain important points not fully described in the text, and others cover new ideas that can be analyzed using tools presented in the current and previous chapters. Several problems require the use of the data sets located at the author's website.},
author = {Wooldridge, Jeffrey M},
booktitle = {booksgooglecom},
doi = {10.1515/humr.2003.021},
isbn = {0262232197},
issn = {09331719},
pages = {752},
title = {{Econometric Analysis of Cross Section and Panel Data}},
url = {http://books.google.com/books?hl=en\&amp;lr=\&amp;id=cdBPOJUP4VsC\&amp;oi=fnd\&amp;pg=PA1\&amp;dq=Econometric+Analysis+of+Cross+Section+and+Panel+Data\&amp;ots=jabbMTk6sf\&amp;sig=lBc6Dsy959N2dTJD42bbNQoBzEY},
volume = {58},
year = {2002}
}
@article{Wright2009,
author = {Wright, Jonathan H.},
doi = {10.1002/for.1088},
file = {:C$\backslash$:/Users/snowdj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wright - 2009 - Forecasting US inflation by Bayesian model averaging.pdf:pdf},
issn = {02776693},
journal = {Journal of Forecasting},
month = mar,
number = {2},
pages = {131--144},
title = {{Forecasting US inflation by Bayesian model averaging}},
url = {http://doi.wiley.com/10.1002/for.1088},
volume = {28},
year = {2009}
}
@article{Zeng2014,
author = {Zeng, Jing},
file = {:E$\backslash$:/Dropbox/phd/shrinkage/WP\_20\_JingZeng\_2014.pdf:pdf},
title = {{Forecasting Aggregates with Disaggregate Variables : Does Boosting Help to Select the Most Relevant Predictors ?}},
year = {2014}
}
@book{,
file = {:C$\backslash$:/Users/snowdj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - Essays on Forecasting and Bayesian Model Averaging.pdf:pdf},
isbn = {9172587105},
title = {{Essays on Forecasting and Bayesian Model Averaging}}
}
@misc{,
keywords = {B Jungbacker \& S J Koopman,Econometrics of Forecasting},
mendeley-tags = {Econometrics of Forecasting},
title = {{Likelihood-Based Dynamic Factor Analysis}},
url = {http://www.fsmevents.com/res/4specialc2/JungbackerKoopmanSlides.pdf},
urldate = {2015-04-28}
}
@misc{,
title = {{cjag\_1163\_LR - AgResEconCompModelingOverview.NolanParkerVanKootenBerger2009.pdf}},
url = {http://www2.econ.iastate.edu/tesfatsi/AgResEconCompModelingOverview.NolanParkerVanKootenBerger2009.pdf},
urldate = {2014-08-15}
}
@article{,
title = {{Cochrane Perm and Trans GNP and stocks (QJE)}}
}
@article{,
journal = {EnANPAD},
number = {3},
pages = {1--16},
title = {{Per Capita}},
year = {2006}
}
